{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8441112b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "# Unstructured for document parsing\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# LangChain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "class PDFPartitioner:\n",
    "    \"\"\"Handles PDF partitioning with configurable options\"\"\"\n",
    "    \n",
    "    # Fixed image output directory\n",
    "    IMAGE_OUTPUT_DIR = r\"D:\\MultiModulRag\\Backend\\SmartChunkClubing\\Images\"\n",
    "    \n",
    "    # Supported languages mapping\n",
    "    SUPPORTED_LANGUAGES = {\n",
    "        \"afrikaans\": \"afr\", \"amharic\": \"amh\", \"arabic\": \"ara\", \"assamese\": \"asm\",\n",
    "        \"azerbaijani\": \"aze\", \"azerbaijani_cyrilic\": \"aze_cyrl\", \"belarusian\": \"bel\",\n",
    "        \"bengali\": \"ben\", \"tibetan\": \"bod\", \"bosnian\": \"bos\", \"breton\": \"bre\",\n",
    "        \"bulgarian\": \"bul\", \"catalan\": \"cat\", \"cebuano\": \"ceb\", \"czech\": \"ces\",\n",
    "        \"chinese_simplified\": \"chi_sim\", \"chinese\": \"chi_sim\", \"chinese_traditional\": \"chi_tra\",\n",
    "        \"cherokee\": \"chr\", \"corsican\": \"cos\", \"welsh\": \"cym\", \"danish\": \"dan\",\n",
    "        \"danish_fraktur\": \"dan_frak\", \"german\": \"deu\", \"german_fraktur\": \"deu_frak\",\n",
    "        \"dzongkha\": \"dzo\", \"greek\": \"ell\", \"english\": \"eng\", \"esperanto\": \"epo\",\n",
    "        \"estonian\": \"est\", \"basque\": \"eus\", \"persian\": \"fas\", \"filipino\": \"fil\",\n",
    "        \"finnish\": \"fin\", \"french\": \"fra\", \"german_fraktur\": \"frk\", \"western_frisian\": \"fry\",\n",
    "        \"scottish_gaelic\": \"gla\", \"irish\": \"gle\", \"galician\": \"glg\", \"gujarati\": \"guj\",\n",
    "        \"haitian\": \"hat\", \"hebrew\": \"heb\", \"hindi\": \"hin\", \"croatian\": \"hrv\",\n",
    "        \"hungarian\": \"hun\", \"armenian\": \"hye\", \"indonesian\": \"ind\", \"icelandic\": \"isl\",\n",
    "        \"italian\": \"ita\", \"javanese\": \"jav\", \"japanese\": \"jpn\", \"kannada\": \"kan\",\n",
    "        \"georgian\": \"kat\", \"kazakh\": \"kaz\", \"khmer\": \"khm\", \"korean\": \"kor\",\n",
    "        \"lao\": \"lao\", \"latin\": \"lat\", \"latvian\": \"lav\", \"lithuanian\": \"lit\",\n",
    "        \"malayalam\": \"mal\", \"marathi\": \"mar\", \"macedonian\": \"mkd\", \"maltese\": \"mlt\",\n",
    "        \"mongolian\": \"mon\", \"malay\": \"msa\", \"burmese\": \"mya\", \"nepali\": \"nep\",\n",
    "        \"dutch\": \"nld\", \"norwegian\": \"nor\", \"polish\": \"pol\", \"portuguese\": \"por\",\n",
    "        \"pashto\": \"pus\", \"romanian\": \"ron\", \"russian\": \"rus\", \"sanskrit\": \"san\",\n",
    "        \"sinhala\": \"sin\", \"slovak\": \"slk\", \"slovenian\": \"slv\", \"spanish\": \"spa\",\n",
    "        \"albanian\": \"sqi\", \"serbian\": \"srp\", \"swedish\": \"swe\", \"tamil\": \"tam\",\n",
    "        \"telugu\": \"tel\", \"thai\": \"tha\", \"turkish\": \"tur\", \"ukrainian\": \"ukr\",\n",
    "        \"urdu\": \"urd\", \"uzbek\": \"uzb\", \"vietnamese\": \"vie\", \"yiddish\": \"yid\"\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        language: str,\n",
    "        max_characters: int,\n",
    "        new_after_n_chars: int,\n",
    "        combine_text_under_n_chars: int,\n",
    "        extract_images: bool = False,\n",
    "        extract_tables: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize PDF Partitioner\n",
    "        \n",
    "        Args:\n",
    "            language: Language name (e.g., 'english', 'spanish', 'hindi') - REQUIRED\n",
    "            max_characters: Maximum characters per chunk - REQUIRED\n",
    "            new_after_n_chars: Start new chunk after this many characters - REQUIRED\n",
    "            combine_text_under_n_chars: Combine small text blocks under this count - REQUIRED\n",
    "            extract_images: Whether to extract images from PDF (default: False)\n",
    "            extract_tables: Whether to parse tables as structured HTML (default: False)\n",
    "        \"\"\"\n",
    "        # Language configuration (required)\n",
    "        self.language = self._validate_language(language)\n",
    "        \n",
    "        # Chunking configuration (all required)\n",
    "        self.max_characters = max_characters\n",
    "        self.new_after_n_chars = new_after_n_chars\n",
    "        self.combine_text_under_n_chars = combine_text_under_n_chars\n",
    "        \n",
    "        # Optional features\n",
    "        self.extract_images = extract_images\n",
    "        self.extract_tables = extract_tables\n",
    "        \n",
    "        # Image output directory\n",
    "        self.image_output_dir = Path(self.IMAGE_OUTPUT_DIR)\n",
    "        \n",
    "        # Create image directory if image extraction is enabled\n",
    "        if self.extract_images:\n",
    "            self.image_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def _validate_language(self, language: str) -> str:\n",
    "        \"\"\"Validate and convert language name to code\"\"\"\n",
    "        lang_lower = language.lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "        \n",
    "        if lang_lower in self.SUPPORTED_LANGUAGES:\n",
    "            return self.SUPPORTED_LANGUAGES[lang_lower]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"‚ùå Unsupported language '{language}'.\\n\"\n",
    "                f\"   Use PDFPartitioner.get_supported_languages() to see valid options.\\n\"\n",
    "                f\"   Examples: english, hindi, spanish, french, etc.\"\n",
    "            )\n",
    "    \n",
    "    def partition_document(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Extract elements from PDF using unstructured\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the PDF file\n",
    "            \n",
    "        Returns:\n",
    "            List of extracted elements\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìÑ Partitioning document: {file_path}\")\n",
    "        print(f\"   Language: {self.language}\")\n",
    "        print(f\"   Extract images: {self.extract_images}\")\n",
    "        print(f\"   Extract tables: {self.extract_tables}\")\n",
    "        print(f\"   Chunking: max={self.max_characters}, new_after={self.new_after_n_chars}, combine_under={self.combine_text_under_n_chars}\")\n",
    "        \n",
    "        # Build partition parameters\n",
    "        partition_params = {\n",
    "            # Core parameters (always required)\n",
    "            \"filename\": file_path,\n",
    "            \"strategy\": \"hi_res\",\n",
    "            \"hi_res_model_name\": \"yolox\",\n",
    "            \"chunking_strategy\": \"by_title\",\n",
    "            \"include_orig_elements\": True,\n",
    "            \n",
    "            # Language configuration\n",
    "            \"languages\": [self.language],\n",
    "            \n",
    "            # Chunking parameters\n",
    "            \"max_characters\": self.max_characters,\n",
    "            \"new_after_n_chars\": self.new_after_n_chars,\n",
    "            \"combine_text_under_n_chars\": self.combine_text_under_n_chars,\n",
    "        }\n",
    "        \n",
    "        # Add image extraction parameters if enabled\n",
    "        if self.extract_images:\n",
    "            partition_params.update({\n",
    "                \"extract_images_in_pdf\": True,\n",
    "                \"extract_image_block_to_payload\": True,\n",
    "                \"extract_image_block_output_dir\": str(self.image_output_dir),\n",
    "                \"extract_image_block_types\": [\"Image\"],\n",
    "            })\n",
    "        \n",
    "        # Add table extraction parameter if enabled\n",
    "        if self.extract_tables:\n",
    "            partition_params[\"infer_table_structure\"] = True\n",
    "        \n",
    "        # Partition the PDF\n",
    "        elements = partition_pdf(**partition_params)\n",
    "        \n",
    "        print(f\"‚úÖ Extracted {len(elements)} elements\\n\")\n",
    "        return elements\n",
    "    \n",
    "    @classmethod\n",
    "    def get_supported_languages(cls) -> list:\n",
    "        \"\"\"Get list of all supported language names\"\"\"\n",
    "        return sorted(cls.SUPPORTED_LANGUAGES.keys())\n",
    "    \n",
    "    @classmethod\n",
    "    def from_terminal_input(cls):\n",
    "        \"\"\"Create PDFPartitioner instance from terminal input with full validation\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\" \" * 20 + \"PDF PARTITIONER CONFIGURATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # ============ PDF FILE PATH ============\n",
    "        print(\"\\nüìÅ PDF FILE PATH:\")\n",
    "        while True:\n",
    "            pdf_path = input(\"  Enter PDF file path: \").strip()\n",
    "            \n",
    "            if not pdf_path:\n",
    "                print(\"  ‚ùå Error: Path cannot be empty. Please try again.\\n\")\n",
    "                continue\n",
    "            \n",
    "            if not os.path.exists(pdf_path):\n",
    "                print(f\"  ‚ùå Error: File does not exist: {pdf_path}\")\n",
    "                retry = input(\"  Do you want to try again? (yes/no): \").strip().lower()\n",
    "                if retry not in ['yes', 'y']:\n",
    "                    raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
    "                continue\n",
    "            \n",
    "            if not pdf_path.lower().endswith('.pdf'):\n",
    "                print(\"  ‚ö†Ô∏è  Warning: File does not have .pdf extension\")\n",
    "                proceed = input(\"  Do you want to proceed anyway? (yes/no): \").strip().lower()\n",
    "                if proceed not in ['yes', 'y']:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"  ‚úÖ File found: {pdf_path}\\n\")\n",
    "            break\n",
    "        \n",
    "        # ============ LANGUAGE CONFIGURATION ============\n",
    "        print(\"üìö LANGUAGE CONFIGURATION:\")\n",
    "        print(f\"  Available languages (showing first 10): {', '.join(cls.get_supported_languages()[:10])}...\")\n",
    "        print(f\"  Total {len(cls.SUPPORTED_LANGUAGES)} languages supported\")\n",
    "        print(\"  Examples: english, hindi, spanish, french, german, japanese, chinese\\n\")\n",
    "        \n",
    "        while True:\n",
    "            language = input(\"  Enter language: \").strip()\n",
    "            \n",
    "            if not language:\n",
    "                print(\"  ‚ùå Error: Language cannot be empty. Please try again.\\n\")\n",
    "                continue\n",
    "            \n",
    "            lang_lower = language.lower().replace(\" \", \"_\").replace(\"-\", \"_\")\n",
    "            if lang_lower not in cls.SUPPORTED_LANGUAGES:\n",
    "                print(f\"  ‚ùå Error: Unsupported language '{language}'\")\n",
    "                print(\"     Type 'list' to see all languages, or try again\")\n",
    "                choice = input(\"  Your choice: \").strip().lower()\n",
    "                \n",
    "                if choice == 'list':\n",
    "                    print(\"\\n  Supported languages:\")\n",
    "                    langs = cls.get_supported_languages()\n",
    "                    for i in range(0, len(langs), 5):\n",
    "                        print(\"    \" + \", \".join(langs[i:i+5]))\n",
    "                    print()\n",
    "                continue\n",
    "            \n",
    "            print(f\"  ‚úÖ Language set: {language}\\n\")\n",
    "            break\n",
    "        \n",
    "        # ============ CHUNKING PARAMETERS ============\n",
    "        print(\"üìè CHUNKING CONFIGURATION (All Required):\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                max_characters = int(input(\"  Max characters per chunk: \").strip())\n",
    "                if max_characters <= 0:\n",
    "                    print(\"  ‚ùå Error: Must be a positive number\\n\")\n",
    "                    continue\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(\"  ‚ùå Error: Please enter a valid integer\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                new_after_n_chars = int(input(\"  Start new chunk after N chars: \").strip())\n",
    "                if new_after_n_chars <= 0:\n",
    "                    print(\"  ‚ùå Error: Must be a positive number\\n\")\n",
    "                    continue\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(\"  ‚ùå Error: Please enter a valid integer\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                combine_text_under_n_chars = int(input(\"  Combine text blocks under N chars: \").strip())\n",
    "                if combine_text_under_n_chars < 0:\n",
    "                    print(\"  ‚ùå Error: Must be a non-negative number\\n\")\n",
    "                    continue\n",
    "                break\n",
    "            except ValueError:\n",
    "                print(\"  ‚ùå Error: Please enter a valid integer\\n\")\n",
    "        \n",
    "        print(f\"  ‚úÖ Chunking configured\\n\")\n",
    "        \n",
    "        # ============ OPTIONAL FEATURES ============\n",
    "        print(\"üîß OPTIONAL FEATURES:\")\n",
    "        \n",
    "        # Image extraction\n",
    "        extract_images = input(\"  Do you want to extract images? (yes/no): \").strip().lower() in ['yes', 'y']\n",
    "        if extract_images:\n",
    "            print(f\"  ‚úÖ Images will be saved to: {cls.IMAGE_OUTPUT_DIR}\")\n",
    "        \n",
    "        # Table extraction\n",
    "        extract_tables = input(\"  Do you want to extract tables? (yes/no): \").strip().lower() in ['yes', 'y']\n",
    "        if extract_tables:\n",
    "            print(\"  ‚úÖ Tables will be extracted as structured HTML\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"‚úÖ Configuration Complete!\")\n",
    "        print(\"=\" * 70 + \"\\n\")\n",
    "        \n",
    "        # Create instance\n",
    "        partitioner = cls(\n",
    "            language=language,\n",
    "            max_characters=max_characters,\n",
    "            new_after_n_chars=new_after_n_chars,\n",
    "            combine_text_under_n_chars=combine_text_under_n_chars,\n",
    "            extract_images=extract_images,\n",
    "            extract_tables=extract_tables\n",
    "        )\n",
    "        \n",
    "        # Return both partitioner and pdf_path\n",
    "        return partitioner, pdf_path\n",
    "    \n",
    "    def get_config_summary(self) -> dict:\n",
    "        \"\"\"Get current configuration as dictionary\"\"\"\n",
    "        return {\n",
    "            \"language\": self.language,\n",
    "            \"extract_images\": self.extract_images,\n",
    "            \"extract_tables\": self.extract_tables,\n",
    "            \"image_output_dir\": str(self.image_output_dir),\n",
    "            \"chunking\": {\n",
    "                \"max_characters\": self.max_characters,\n",
    "                \"new_after_n_chars\": self.new_after_n_chars,\n",
    "                \"combine_text_under_n_chars\": self.combine_text_under_n_chars\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# ‚úÖ USAGE EXAMPLE - Interactive Terminal Mode\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Get configuration and PDF path from terminal\n",
    "        partitioner, pdf_path = PDFPartitioner.from_terminal_input()\n",
    "        \n",
    "        # Process the document\n",
    "        elements = partitioner.partition_document(pdf_path)\n",
    "        \n",
    "        print(\"üéâ Processing completed successfully!\")\n",
    "        print(f\"üìä Summary: {len(elements)} elements extracted\")\n",
    "\n",
    "        # Define output directories\n",
    "        json_dir = r\"D:\\MultiModulRag\\Backend\\SmartChunkClubing\\JSON\"\n",
    "        pickle_dir = r\"D:\\MultiModulRag\\Backend\\SmartChunkClubing\\Pickel\"\n",
    "\n",
    "        # Create directories if they don‚Äôt exist\n",
    "        os.makedirs(json_dir, exist_ok=True)\n",
    "        os.makedirs(pickle_dir, exist_ok=True)\n",
    "\n",
    "        # Extract filename (without extension)\n",
    "        base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "        # Define output paths\n",
    "        json_path = os.path.join(json_dir, f\"{base_name}.json\")\n",
    "        pickle_path = os.path.join(pickle_dir, f\"{base_name}.pkl\")\n",
    "\n",
    "        # Dump to JSON\n",
    "        try:\n",
    "            with open(json_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "                json.dump(\n",
    "                    [el.to_dict() if hasattr(el, \"to_dict\") else str(el) for el in elements],\n",
    "                    jf,\n",
    "                    indent=4,\n",
    "                    ensure_ascii=False\n",
    "                )\n",
    "            print(f\"‚úÖ JSON saved at: {json_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving JSON: {e}\")\n",
    "\n",
    "        # Dump to Pickle\n",
    "        try:\n",
    "            with open(pickle_path, \"wb\") as pf:\n",
    "                pickle.dump(elements, pf)\n",
    "            print(f\"‚úÖ Pickle saved at: {pickle_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving Pickle: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üö® Error: {e}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n‚ùå File Error: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n‚ùå Configuration Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Unexpected Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5fec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from unstructured.documents.elements import Element\n",
    "\n",
    "def save_elements(elements, pkl_path: str, json_path: str = None):\n",
    "    \"\"\"\n",
    "    Save a Python variable `elements` to pickle and optionally to JSON.\n",
    "    Automatically converts unstructured Element objects to dicts for JSON.\n",
    "\n",
    "    Args:\n",
    "        elements: Python variable to save (list, dict, etc.)\n",
    "        pkl_path: Path to save the pickle file (required)\n",
    "        json_path: Path to save the JSON file (optional)\n",
    "    \"\"\"\n",
    "    # Ensure parent directories exist\n",
    "    Path(pkl_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    if json_path:\n",
    "        Path(json_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save as Pickle\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(elements, f)\n",
    "    print(f\"‚úÖ Saved elements to pickle: {pkl_path}\")\n",
    "\n",
    "    # Save as JSON (optional)\n",
    "    if json_path:\n",
    "        # Convert Element objects to dicts automatically\n",
    "        def to_serializable(el):\n",
    "            return el.to_dict() if isinstance(el, Element) else el\n",
    "        \n",
    "        elements_serializable = [to_serializable(el) for el in elements]\n",
    "\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(elements_serializable, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"‚úÖ Saved elements to JSON: {json_path}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# your Python variable, e.g., output of partition_pdf\n",
    "\n",
    "pkl_file = r\"D:\\MultiModulRag\\Backend\\Pipeline_Database\\Pickel\\Checkpointer1.pkl\"\n",
    "json_file = r\"D:\\MultiModulRag\\Backend\\Pipeline_Database\\JSON\\Checkpointer1.json\"\n",
    "\n",
    "save_elements(elements, pkl_file, json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c71d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Processing chunk 1/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 2/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 3/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 4/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 5/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 6/24\n",
      "     Types found: ['text', 'image']\n",
      "     Tables: 0, Images: 1\n",
      "   Processing chunk 7/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 8/24\n",
      "     Types found: ['text', 'image']\n",
      "     Tables: 0, Images: 2\n",
      "   Processing chunk 9/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 10/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 11/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 12/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 13/24\n",
      "     Types found: ['text', 'table']\n",
      "     Tables: 1, Images: 0\n",
      "   Processing chunk 14/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 15/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 16/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 17/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 18/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 19/24\n",
      "     Types found: ['text', 'table']\n",
      "     Tables: 1, Images: 0\n",
      "   Processing chunk 20/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 21/24\n",
      "     Types found: ['text', 'table']\n",
      "     Tables: 1, Images: 0\n",
      "   Processing chunk 22/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 23/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "   Processing chunk 24/24\n",
      "     Types found: ['text']\n",
      "     Tables: 0, Images: 0\n",
      "Total images: 3\n",
      "Total tables: 3\n",
      "Total text chunks: 24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "class ContentSeparator:\n",
    "    \"\"\"Handles separation and storage of different content types from chunks\"\"\"\n",
    "    \n",
    "    def __init__(self, image_dir):\n",
    "        \"\"\"\n",
    "        Initialize the content separator\n",
    "        \n",
    "        Args:\n",
    "            image_dir: Directory path where images will be saved\n",
    "        \"\"\"\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.image_counter = 1\n",
    "        \n",
    "        # Setup directory\n",
    "        self.image_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self._clear_existing_images()\n",
    "    \n",
    "    def _clear_existing_images(self):\n",
    "        \"\"\"Private method to clear existing images in directory\"\"\"\n",
    "        for file in self.image_dir.glob(\"*\"):\n",
    "            if file.is_file():\n",
    "                file.unlink()\n",
    "    \n",
    "    def separate_content_types(self, chunk):\n",
    "        \"\"\"\n",
    "        Analyze and extract content types from a chunk\n",
    "        \n",
    "        Args:\n",
    "            chunk: The chunk object to process\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing separated content\n",
    "        \"\"\"\n",
    "        content_data = {\n",
    "            'text': chunk.text,\n",
    "            'tables': [],\n",
    "            'images_base64': [],\n",
    "            'images_dirpath': [],\n",
    "            'page_no': [],\n",
    "            'types': ['text']\n",
    "        }\n",
    "\n",
    "        if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
    "            for element in chunk.metadata.orig_elements:\n",
    "                element_type = type(element).__name__\n",
    "                \n",
    "                # Handle page numbers\n",
    "                page_no = element.to_dict()['metadata']['page_number']\n",
    "                if page_no not in content_data['page_no']: \n",
    "                    content_data['page_no'].append(page_no)\n",
    "                \n",
    "                # Handle tables\n",
    "                if element_type == 'Table':\n",
    "                    self.process_table(element, content_data)\n",
    "                \n",
    "                # Handle images\n",
    "                elif element_type == 'Image':\n",
    "                    self.process_image(element, content_data)\n",
    "\n",
    "        return content_data\n",
    "    \n",
    "    def process_table(self, element, content_data):\n",
    "        \"\"\"Private method to process table elements\"\"\"\n",
    "        if 'table' not in content_data['types']:\n",
    "            content_data['types'].append('table')\n",
    "        table_html = getattr(element.metadata, 'text_as_html', element.text)\n",
    "        content_data['tables'].append(table_html)\n",
    "    \n",
    "    def process_image(self, element, content_data):\n",
    "        \"\"\"Private method to process image elements\"\"\"\n",
    "        if not (hasattr(element, 'metadata') and hasattr(element.metadata, 'image_base64')):\n",
    "            return\n",
    "        \n",
    "        if 'image' not in content_data['types']:\n",
    "            content_data['types'].append('image')\n",
    "        \n",
    "        image_base64 = element.metadata.image_base64\n",
    "        content_data['images_base64'].append(image_base64)\n",
    "        \n",
    "        try:\n",
    "            image_filename = f\"image_{self.image_counter}.png\"\n",
    "            image_path = self.image_dir / image_filename\n",
    "            \n",
    "            with open(image_path, \"wb\") as img_file:\n",
    "                img_file.write(base64.b64decode(image_base64))\n",
    "            \n",
    "            content_data['images_dirpath'].append(str(image_path))\n",
    "            # print(f\"     ‚úÖ Saved image: {image_filename}\")\n",
    "            \n",
    "            self.image_counter += 1  # Increment instance counter\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ‚ùå Failed to save image {self.image_counter}: {e}\")\n",
    "    \n",
    "    def process_chunks(self, chunks):\n",
    "        \"\"\"\n",
    "        Process multiple chunks\n",
    "        \n",
    "        Args:\n",
    "            chunks: List of chunks to process\n",
    "            \n",
    "        Returns:\n",
    "            list: List of content data dictionaries\n",
    "        \"\"\"\n",
    "        all_content_data = []\n",
    "        total_chunks = len(chunks)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            current_chunk = i + 1\n",
    "            print(f\"   Processing chunk {current_chunk}/{total_chunks}\")\n",
    "            \n",
    "            content_data = self.separate_content_types(chunk)\n",
    "            \n",
    "            print(f\"     Types found: {content_data['types']}\")\n",
    "            print(f\"     Tables: {len(content_data['tables'])}, Images: {len(content_data['images_base64'])}\")\n",
    "            \n",
    "            all_content_data.append(content_data)\n",
    "        \n",
    "        # print(f\"\\n Total images saved: {self.image_counter - 1}\")\n",
    "        return all_content_data\n",
    "    \n",
    "    def get_table_count(self, content_data):\n",
    "        \"\"\"Get the total number of tables processed\"\"\"\n",
    "        return sum(len(data['tables']) for data in content_data)\n",
    "    \n",
    "    def get_text_count(self, content_data):\n",
    "        \"\"\"Get the total number of text chunks processed\"\"\"\n",
    "        return sum(1 for data in content_data if 'text' in data['types'])\n",
    "\n",
    "    # def get_image_count(self):\n",
    "    #     \"\"\"Get the total number of images processed\"\"\"\n",
    "    #     # Count actual files in directory (source of truth)\n",
    "    #     return len(list(self.image_dir.glob(\"*.png\")))\n",
    "    \n",
    "    def get_image_count(self, content_data):\n",
    "        \"\"\"Get the total number of images processed\"\"\"\n",
    "        return sum(len(data['images_base64']) for data in content_data)\n",
    "\n",
    "# ‚úÖ USAGE - Clean and Simple!\n",
    "if __name__ == \"__main__\":\n",
    "    # Create an instance\n",
    "    separator = ContentSeparator(\n",
    "        image_dir=r\"D:\\MultiModulRag\\Backend\\Pipeline_Database\\Images\"\n",
    "    )\n",
    "    \n",
    "    # Process all chunks\n",
    "    all_content_data = separator.process_chunks(checkpoint)\n",
    "    \n",
    "    # Get stats\n",
    "    print(f\"Total images: {separator.get_image_count(all_content_data)}\")\n",
    "    print(f\"Total tables: {separator.get_table_count(all_content_data)}\")\n",
    "    print(f\"Total text chunks: {separator.get_text_count(all_content_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d1205ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total images: {separator.get_image_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d28572a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c2bdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '1 Introduction\\n\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been Ô¨Årmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].\\n\\n‚àóEqual contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the Ô¨Årst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efÔ¨Åcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\\n\\n‚Ä†Work performed while at Google Brain.\\n\\n‚Ä°Work performed while at Google Research.\\n\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved signiÔ¨Åcant improvements in computational efÔ¨Åciency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.\\n\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.\\n\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for signiÔ¨Åcantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.',\n",
       " 'tables': [],\n",
       " 'images_base64': [],\n",
       " 'images_dirpath': [],\n",
       " 'page_no': [1, 2],\n",
       " 'types': ['text']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_content_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e74fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def partition_document(file_path: str):\n",
    "    \"\"\"Extract elements from PDF using unstructured\"\"\"\n",
    "    print(f\"üìÑ Partitioning document: {file_path}\")\n",
    "    path = r\"D:\\MultiModulRag\\Backend\\SmartChunkClubing\\Images\"\n",
    "    \n",
    "    # elements = partition_pdf(\n",
    "    # filename=file_path, # Stores the path to the PDF file\n",
    "    # strategy = \"hi_res\", # Uses high-resolution strategy for better accuracy\n",
    "    # languages=[\"eng\"], # Specifies the language as English\n",
    "    # extract_images_in_pdf=True, # Enables image extraction from the PDF but is deprecated and in future will use 'extract_image_block_types'\n",
    "    #     # extract_image_block_types=\n",
    "    #     # Only applicable if `strategy=hi_res`.\n",
    "    #     # Images of the element type(s) specified in this list (e.g., [\"Image\", \"Table\"]) will be\n",
    "    #     # saved in the path specified by 'extract_image_block_output_dir' or stored as base64\n",
    "    #     # encoded data within metadata fields.\n",
    "    # # include_page_breaks=True, # Includes page breaks in the output \n",
    "    # chunking_strategy=\"by_title\", # Chunks content based on titles\n",
    "    # extract_image_block_types=[\"Image\"], # Specifies types of image blocks to extract\n",
    "    # hi_res_model_name=\"yolox\", # Specifies the model for high-resolution extraction\n",
    "    # infer_table_structure=True, # Keep tables as structured HTML, not jumbled text\n",
    "    # extract_image_block_output_dir=path, # Directory to save extracted images\n",
    "    # extract_image_block_to_payload=True, # Saves image data in the element payload\n",
    "    \n",
    "    # ### ***Combining chunk parameter*** ###\n",
    "    # max_characters=3000, # Maximum characters per chunk\n",
    "    # new_after_n_chars=3800, # Starts a new chunk after this many characters\n",
    "    # combine_text_under_n_chars=200, # Combines small text blocks under this character count\n",
    "    # include_orig_elements=True # Includes original elements in the output\n",
    "    # )\n",
    "    elements = partition_pdf(\n",
    "\n",
    "        ### ***Core parameters(Fixed Parameters)()*** ###\n",
    "        filename=file_path,\n",
    "        strategy = \"hi_res\",\n",
    "        hi_res_model_name=\"yolox\",\n",
    "        chunking_strategy=\"by_title\",\n",
    "        include_orig_elements=True # Includes original elements in the output\n",
    "\n",
    "        ### ***Language and extraction parameters (must select this)*** ###\n",
    "        languages=[], # Specifies the language and must select this\n",
    "        ### Supported languages are:\n",
    "            # \"afrikaans\": \"afr\",\n",
    "            # \"amharic\": \"amh\",\n",
    "            # \"arabic\": \"ara\",\n",
    "            # \"assamese\": \"asm\",\n",
    "            # \"azerbaijani\": \"aze\",\n",
    "            # \"azerbaijani - cyrilic\": \"aze_cyrl\",\n",
    "            # \"belarusian\": \"bel\",\n",
    "            # \"bengali\": \"ben\",\n",
    "            # \"tibetan\": \"bod\",\n",
    "            # \"bosnian\": \"bos\",\n",
    "            # \"breton\": \"bre\",\n",
    "            # \"bulgarian\": \"bul\",\n",
    "            # \"catalan; Valencian\": \"cat\",\n",
    "            # \"cebuano\": \"ceb\",\n",
    "            # \"czech\": \"ces\",\n",
    "            # \"chinese - simplified\": \"chi_sim\",\n",
    "            # \"chinese\": \"chi_sim\",\n",
    "            # \"chinese - traditional\": \"chi_tra\",\n",
    "            # \"cherokee\": \"chr\",\n",
    "            # \"corsican\": \"cos\",\n",
    "            # \"welsh\": \"cym\",\n",
    "            # \"danish\": \"dan\",\n",
    "            # \"danish - fraktur\": \"dan_frak\",\n",
    "            # \"german\": \"deu\",\n",
    "            # \"german - fraktur (contrib)\": \"deu_frak\",  # \"contrib\" not removed because it would repeat key\n",
    "            # \"dzongkha\": \"dzo\",\n",
    "            # \"greek, modern\": \"ell\",\n",
    "            # \"greek\": \"ell\",\n",
    "            # \"english\": \"eng\",\n",
    "            # \"english, middle\": \"enm\",\n",
    "            # \"esperanto\": \"epo\",\n",
    "            # \"math / equation detection module\": \"equ\",\n",
    "            # \"estonian\": \"est\",\n",
    "            # \"basque\": \"eus\",\n",
    "            # \"faroese\": \"fao\",\n",
    "            # \"persian\": \"fas\",\n",
    "            # \"filipino (old - tagalog)\": \"fil\",\n",
    "            # \"filipino\": \"fil\",\n",
    "            # \"finnish\": \"fin\",\n",
    "            # \"french\": \"fra\",\n",
    "            # \"german - fraktur\": \"frk\",\n",
    "            # \"french, middle\": \"frm\",\n",
    "            # \"western frisian\": \"fry\",\n",
    "            # \"scottish gaelic\": \"gla\",\n",
    "            # \"irish\": \"gle\",\n",
    "            # \"galician\": \"glg\",\n",
    "            # \"greek, ancient\": \"grc\",\n",
    "            # \"gujarati\": \"guj\",\n",
    "            # \"haitian\": \"hat\",\n",
    "            # \"haitian creole\": \"hat\",\n",
    "            # \"hebrew\": \"heb\",\n",
    "            # \"hindi\": \"hin\",\n",
    "            # \"croatian\": \"hrv\",\n",
    "            # \"hungarian\": \"hun\",\n",
    "            # \"armenian\": \"hye\",\n",
    "            # \"inuktitut\": \"iku\",\n",
    "            # \"indonesian\": \"ind\",\n",
    "            # \"icelandic\": \"isl\",\n",
    "            # \"italian\": \"ita\",\n",
    "            # \"italian - old\": \"ita_old\",\n",
    "            # \"javanese\": \"jav\",\n",
    "            # \"japanese\": \"jpn\",\n",
    "            # \"kannada\": \"kan\",\n",
    "            # \"georgian\": \"kat\",\n",
    "            # \"georgian - old\": \"kat_old\",\n",
    "            # \"kazakh\": \"kaz\",\n",
    "            # \"central khmer\": \"khm\",\n",
    "            # \"kirghiz\": \"kir\",\n",
    "            # \"kyrgyz\": \"kir\",\n",
    "            # \"kurmanji (kurdish - latin script)\": \"kmr\",\n",
    "            # \"korean\": \"kor\",\n",
    "            # \"korean (vertical)\": \"kor_vert\",\n",
    "            # \"kurdish (arabic script)\": \"kur\",\n",
    "            # \"lao\": \"lao\",\n",
    "            # \"latin\": \"lat\",\n",
    "            # \"latvian\": \"lav\",\n",
    "            # \"lithuanian\": \"lit\",\n",
    "            # \"luxembourgish\": \"ltz\",\n",
    "            # \"malayalam\": \"mal\",\n",
    "            # \"marathi\": \"mar\",\n",
    "            # \"macedonian\": \"mkd\",\n",
    "            # \"maltese\": \"mlt\",\n",
    "            # \"mongolian\": \"mon\",\n",
    "            # \"maori\": \"mri\",\n",
    "            # \"malay\": \"msa\",\n",
    "            # \"burmese\": \"mya\",\n",
    "            # \"nepali\": \"nep\",\n",
    "            # \"dutch\": \"nld\",\n",
    "            # \"flemish\": \"nld\",\n",
    "            # \"norwegian\": \"nor\",\n",
    "            # \"occitan\": \"oci\",\n",
    "            # \"oriya\": \"ori\",\n",
    "            # \"orientation and script detection module\": \"osd\",\n",
    "            # \"panjabi\": \"pan\",\n",
    "            # \"punjabi\": \"pan\",\n",
    "            # \"polish\": \"pol\",\n",
    "            # \"portuguese\": \"por\",\n",
    "            # \"pushto\": \"pus\",\n",
    "            # \"pashto\": \"pus\",\n",
    "            # \"quechua\": \"que\",\n",
    "            # \"romanian\": \"ron\",\n",
    "            # \"moldavian\": \"ron\",\n",
    "            # \"moldovan\": \"ron\",\n",
    "            # \"russian\": \"rus\",\n",
    "            # \"sanskrit\": \"san\",\n",
    "            # \"sinhala\": \"sin\",\n",
    "            # \"sinhalese\": \"sin\",\n",
    "            # \"slovak\": \"slk\",\n",
    "            # \"slovak - fraktur\": \"slk_frak\",\n",
    "            # \"slovenian\": \"slv\",\n",
    "            # \"sindhi\": \"snd\",\n",
    "            # \"spanish\": \"spa\",\n",
    "            # \"castilian\": \"spa\",\n",
    "            # \"spanish - old\": \"spa_old\",\n",
    "            # \"castilian - old\": \"spa_old\",\n",
    "            # \"albanian\": \"sqi\",\n",
    "            # \"serbian\": \"srp\",\n",
    "            # \"serbian - latin\": \"srp_latn\",\n",
    "            # \"sundanese\": \"sun\",\n",
    "            # \"swahili\": \"swa\",\n",
    "            # \"swedish\": \"swe\",\n",
    "            # \"syriac\": \"syr\",\n",
    "            # \"tamil\": \"tam\",\n",
    "            # \"tatar\": \"tat\",\n",
    "            # \"telugu\": \"tel\",\n",
    "            # \"tajik\": \"tgk\",\n",
    "            # \"tagalog\": \"tgl\",\n",
    "            # \"thai\": \"tha\",\n",
    "            # \"tigrinya\": \"tir\",\n",
    "            # \"tonga\": \"ton\",\n",
    "            # \"turkish\": \"tur\",\n",
    "            # \"uighur\": \"uig\",\n",
    "            # \"uyghur\": \"uig\",\n",
    "            # \"ukrainian\": \"ukr\",\n",
    "            # \"urdu\": \"urd\",\n",
    "            # \"uzbek\": \"uzb\",\n",
    "            # \"uzbek - cyrilic\": \"uzb_cyrl\",\n",
    "            # \"vietnamese\": \"vie\",\n",
    "            # \"yiddish\": \"yid\",\n",
    "            # \"yoruba\": \"yor\",\n",
    "\n",
    "\n",
    "        ### (OPTIONAL) ***Image extraction parameters(if user want to extract image both \n",
    "        ### ***'extract_images_in_pdf' and 'extract_image_block_to_payload' will be true)*** ###\n",
    "        extract_images_in_pdf=True, # extract image from pdf\n",
    "        extract_image_block_to_payload=True, \n",
    "        extract_image_block_output_dir=path,\n",
    "        extract_image_block_types=[\"Image\"], # this will be set to \"Image\" if user want to extract image from pdf\n",
    "\n",
    "\n",
    "        ### (OPTIONAL) ***Table ( If user wnat it will be true)*** ###\n",
    "        infer_table_structure=True, # Keep tables as structured HTML, not jumbled text\n",
    "\n",
    "\n",
    "        ### (Always Wanted and Asked) ***Chunk Parameter*** ###\n",
    "        max_characters=3000, # Maximum characters per chunk\n",
    "        new_after_n_chars=3800, # Starts a new chunk after this many characters\n",
    "        combine_text_under_n_chars=200, # Combines small text blocks under this character count\n",
    "\n",
    "    )\n",
    "    print(f\"‚úÖ Extracted {len(elements)} elements\")\n",
    "    return elements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
