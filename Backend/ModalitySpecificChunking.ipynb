{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bea030",
   "metadata": {},
   "source": [
    "# System Dependencies\n",
    "\n",
    "To work with **Unstructured.io** and process documents effectively, a few system-wide dependencies are required. These tools handle PDFs, images, and file type detection.\n",
    "\n",
    "---\n",
    "\n",
    "## Poppler (poppler-utils) ðŸ–¥ï¸\n",
    "\n",
    "**Description:**  \n",
    "Poppler is a **collection of command-line tools** for PDF processing. It can extract **text**, **images**, and **metadata** from PDFs.  \n",
    "\n",
    "**Use in Project:**  \n",
    "Unstructured uses Poppler to parse PDF documents and convert them into processable text or extract embedded images.  \n",
    "\n",
    "**Common CLI Tools:**  \n",
    "| Tool | Function |\n",
    "|------|---------|\n",
    "| `pdftotext` | Extracts selectable text from PDFs |\n",
    "| `pdfimages` | Extracts embedded images from PDFs |\n",
    "| `pdftoppm` / `pdftocairo` | Converts PDF pages to images (useful for OCR) |\n",
    "\n",
    "---\n",
    "\n",
    "## Tesseract (tesseract-ocr) ðŸ”\n",
    "\n",
    "**Description:**  \n",
    "Tesseract is an **Optical Character Recognition (OCR) engine**. It can read text from **scanned documents**, **images containing text**, or **image-based PDFs** and convert it into machine-readable text.  \n",
    "\n",
    "**Use in Project:**  \n",
    "- Extract text from scanned PDFs or image-heavy documents  \n",
    "- OCR diagrams or images with embedded text  \n",
    "- Works alongside tools like pdf2image or Poppler for converting pages to images before OCR  \n",
    "\n",
    "---\n",
    "\n",
    "## libmagic / python-magic-bin ðŸ› ï¸\n",
    "\n",
    "**Description:**  \n",
    "libmagic is a **file type detection library**. It identifies the actual type of a file (PDF, Word, image, etc.) by analyzing its content rather than just relying on the file extension.  \n",
    "\n",
    "**Use in Project:**  \n",
    "- Helps Unstructured automatically determine the correct processing method for each document  \n",
    "- On Windows, use **python-magic-bin**, a Python-friendly wrapper around libmagic  \n",
    "\n",
    "---\n",
    "\n",
    "âœ… **Summary:**  \n",
    "For your project using Unstructured.io:  \n",
    "1. **Poppler** â†’ PDF text, images, and metadata extraction  \n",
    "2. **Tesseract OCR** â†’ Text extraction from scanned pages and images  \n",
    "3. **libmagic** â†’ File type detection to route documents to the appropriate processing method  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f25b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# for linux\n",
    "# !apt-get install poppler-utils tesseract-ocr libmagic-dev\n",
    "\n",
    "# for mac\n",
    "# !brew install poppler tesseract libmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fddda27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unstructured-client in d:\\multimodulrag\\.venv\\lib\\site-packages (0.42.3)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in d:\\multimodulrag\\.venv\\lib\\site-packages (from unstructured-client) (25.1.0)\n",
      "Requirement already satisfied: cryptography>=3.1 in d:\\multimodulrag\\.venv\\lib\\site-packages (from unstructured-client) (46.0.3)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in d:\\multimodulrag\\.venv\\lib\\site-packages (from unstructured-client) (1.0.9)\n",
      "Requirement already satisfied: httpx>=0.27.0 in d:\\multimodulrag\\.venv\\lib\\site-packages (from unstructured-client) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in d:\\multimodulrag\\.venv\\lib\\site-packages (from unstructured-client) (2.12.3)\n",
      "Requirement already satisfied: pypdf>=4.0 in d:\\multimodulrag\\.venv\\lib\\site-packages (from unstructured-client) (6.1.2)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\multimodulrag\\.venv\\lib\\site-packages (from unstructured-client) (1.0.0)\n",
      "Requirement already satisfied: cffi>=2.0.0 in d:\\multimodulrag\\.venv\\lib\\site-packages (from cryptography>=3.1->unstructured-client) (2.0.0)\n",
      "Requirement already satisfied: pycparser in d:\\multimodulrag\\.venv\\lib\\site-packages (from cffi>=2.0.0->cryptography>=3.1->unstructured-client) (2.23)\n",
      "Requirement already satisfied: certifi in d:\\multimodulrag\\.venv\\lib\\site-packages (from httpcore>=1.0.9->unstructured-client) (2025.10.5)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\multimodulrag\\.venv\\lib\\site-packages (from httpcore>=1.0.9->unstructured-client) (0.16.0)\n",
      "Requirement already satisfied: anyio in d:\\multimodulrag\\.venv\\lib\\site-packages (from httpx>=0.27.0->unstructured-client) (4.11.0)\n",
      "Requirement already satisfied: idna in d:\\multimodulrag\\.venv\\lib\\site-packages (from httpx>=0.27.0->unstructured-client) (3.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\multimodulrag\\.venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in d:\\multimodulrag\\.venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in d:\\multimodulrag\\.venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\multimodulrag\\.venv\\lib\\site-packages (from pydantic>=2.11.2->unstructured-client) (0.4.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.1 in d:\\multimodulrag\\.venv\\lib\\site-packages (from requests-toolbelt>=1.0.0->unstructured-client) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\multimodulrag\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=1.0.0->unstructured-client) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\multimodulrag\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.1->requests-toolbelt>=1.0.0->unstructured-client) (2.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\multimodulrag\\.venv\\lib\\site-packages (from anyio->httpx>=0.27.0->unstructured-client) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f78e76",
   "metadata": {},
   "source": [
    "### BY API CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144884d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "# Unstructured for document parsing\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# LangChain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI,GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a836f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNSTRUCTURED_API_KEY = \"3HiBFzIghWShFPNY2q5JRzkKsabnCS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78fb6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "import unstructured_client\n",
    "from unstructured_client.models import shared, errors\n",
    "\n",
    "client = unstructured_client.UnstructuredClient(\n",
    "    api_key_auth=UNSTRUCTURED_API_KEY\n",
    ")\n",
    "\n",
    "async def partition_file_via_api(filename):\n",
    "    req = {\n",
    "        \"partition_parameters\": {\n",
    "            \"files\": {\n",
    "                \"content\": open(filename, \"rb\"),\n",
    "                \"file_name\": os.path.basename(filename),\n",
    "            },\n",
    "            \"strategy\": shared.Strategy.AUTO,\n",
    "            \"vlm_model\": \"gpt-4o\",\n",
    "            \"vlm_model_provider\": \"openai\",\n",
    "            \"languages\": ['eng'],\n",
    "            \"split_pdf_page\": True,\n",
    "            \"split_pdf_allow_failed\": True,\n",
    "            \"split_pdf_concurrency_level\": 15\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        res = await client.general.partition_async(request=req)\n",
    "        return res.elements\n",
    "    except errors.UnstructuredClientError as e:\n",
    "        print(f\"Error partitioning {filename}: {e.message}\")\n",
    "        return []\n",
    "\n",
    "async def process_file_and_save_result(input_filename, output_dir):\n",
    "    elements = await partition_file_via_api(input_filename)\n",
    "\n",
    "    if elements:\n",
    "        results_name = f\"{os.path.basename(input_filename)}.json\"\n",
    "        output_filename = os.path.join(output_dir, results_name)\n",
    "\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            json.dump(elements, f, indent=4)\n",
    "\n",
    "def load_filenames_in_directory(input_dir):\n",
    "    filenames = []\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith('.json'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "\n",
    "    return filenames\n",
    "\n",
    "async def process_files():\n",
    "    input_dir = \"./input/\"\n",
    "    output_dir = \"./output/\"\n",
    "\n",
    "    filenames = load_filenames_in_directory(input_dir)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    tasks = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        tasks.append(\n",
    "            process_file_and_save_result(filename, output_dir)\n",
    "        )\n",
    "\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "await process_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f2e2366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: HTTP Request: GET https://api.unstructuredapp.io/general/docs \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://api.unstructuredapp.io/general/v0/general \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved JSON to: D:\\MultiModulRag\\docs\\NIPS-2017-attention-is-all-you-need-Paper.pdf.json\n",
      "Number of elements extracted: 170\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import json\n",
    "import unstructured_client\n",
    "from unstructured_client.models import shared, errors\n",
    "\n",
    "# Initialize the client\n",
    "client = unstructured_client.UnstructuredClient(\n",
    "    api_key_auth=UNSTRUCTURED_API_KEY\n",
    ")\n",
    "\n",
    "# Declare global variable\n",
    "elements = []\n",
    "\n",
    "async def extract_pdf_to_json(pdf_path: str, output_dir: str):\n",
    "    \"\"\"\n",
    "    Extracts content from a PDF using Unstructured API\n",
    "    and saves it as a JSON file in the output directory.\n",
    "    \"\"\"\n",
    "    global elements  # <-- declare that we will modify the global variable\n",
    "\n",
    "    # Prepare request\n",
    "    req = {\n",
    "        \"partition_parameters\": {\n",
    "            \"files\": {\n",
    "                \"content\": open(pdf_path, \"rb\"), # File path\n",
    "                \"file_name\": os.path.basename(pdf_path), # File name\n",
    "            },\n",
    "            \"strategy\": shared.Strategy.AUTO, # Strategy for Auto use \"shared.Strategy.AUTO\" for High perfomance use \"hi_res\"\n",
    "            \"vlm_model\": \"gpt-4o\", # LLM model\n",
    "            \"vlm_model_provider\": \"openai\", # LLM provider\n",
    "            \"languages\": [\"eng\"], # Language\n",
    "            \"split_pdf_page\": True, # Split by page\n",
    "            \"split_pdf_allow_failed\": True, # Allow failed splits\n",
    "            \"split_pdf_concurrency_level\": 15 ,# Concurrency level\n",
    "            \"extract_image_block_to_payload\": True # Extract images as base64\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Try extracting content\n",
    "    try:\n",
    "        res = await client.general.partition_async(request=req)\n",
    "        elements = res.elements  # <-- modify global variable\n",
    "    except errors.UnstructuredClientError as e:\n",
    "        print(f\"Error partitioning {pdf_path}: {e.message}\")\n",
    "        elements = []  # <-- also modifies global\n",
    "\n",
    "    # Save result\n",
    "    if elements:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_filename = os.path.join(output_dir, f\"{os.path.basename(pdf_path)}.json\")\n",
    "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(elements, f, indent=4)\n",
    "        print(f\"âœ… Saved JSON to: {output_filename}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ No elements extracted from {pdf_path}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "pdf_file = r\"D:\\MultiModulRag\\docs\\NIPS-2017-attention-is-all-you-need-Paper.pdf\"\n",
    "output_folder = r\"D:\\MultiModulRag\\docs\"\n",
    "\n",
    "# Run the async function\n",
    "await extract_pdf_to_json(pdf_file, output_folder)\n",
    "\n",
    "# Now 'elements' is available globally\n",
    "print(f\"Number of elements extracted: {len(elements)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "802f4cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UncategorizedText', 'PageNumber', 'Table', 'Footer', 'Title', 'Formula', 'Image', 'NarrativeText']\n"
     ]
    }
   ],
   "source": [
    "# All types of different atomic elements we see from unstructured\n",
    "unique_types = list({item['type'] for item in elements})\n",
    "\n",
    "print(unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b3cd26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'PageNumber',\n",
       " 'element_id': 'e66197b013c34f8553b126ada6b033e4',\n",
       " 'text': '2',\n",
       " 'metadata': {'filetype': 'application/pdf',\n",
       "  'languages': ['eng'],\n",
       "  'page_number': 2,\n",
       "  'filename': 'NIPS-2017-attention-is-all-you-need-Paper.pdf'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7abe8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42 URLs:\n",
      "['figure.1', 'cite.neural_gpu', 'subsubsection.3.2.2', 'cite.extendedngpu', 'cite.DBLP:journals/corr/BritzGLL17', 'cite.kingma2014adam', 'cite.sutskever14', 'cite.lin2017structured', 'cite.sukhbaatar2015', 'cite.luong2015effective', 'cite.paulus2017deep', 'cite.sennrich2015neural', 'cite.wu2016google', 'cite.DBLP:journals/corr/SzegedyVISW15', 'cite.srivastava2014dropout', 'cite.shazeer2017outrageously', 'cite.he2016deep', 'figure.2', 'Hfootnote.2', 'cite.Kuchaiev2017Factorization', 'cite.structuredAttentionNetworks', 'cite.decomposableAttnModel', 'table.1', 'subsection.3.2', 'cite.bahdanau2014neural', 'cite.layernorm2016', 'table.3', 'table.2', 'cite.press2016using', 'cite.DBLP:journals/corr/ZhouCWLX16', 'https://github.com/tensorflow/tensor2tensor', 'cite.gruEval14', 'cite.JonasFaceNet2017', 'cite.xception2016', 'cite.graves2013generating', 'cite.NalBytenet2017', 'cite.hochreiter1997', 'cite.jozefowicz2016exploring', 'cite.hochreiter2001gradient', 'Hfootnote.1', 'cite.cho2014learning', 'cite.cheng2016long']\n"
     ]
    }
   ],
   "source": [
    "# Gather all images\n",
    "# Check the attribute for each element\n",
    "\n",
    "# Often the type property is called 'element_type'\n",
    "# Collect all URLs from NarrativeText elements\n",
    "# Collect all URLs from all elements\n",
    "urls = []\n",
    "\n",
    "for el in elements:\n",
    "    # Safely get metadata dictionary\n",
    "    metadata = el.get(\"metadata\", {})\n",
    "    # Get links list if it exists\n",
    "    links = metadata.get(\"links\", [])\n",
    "    for link in links:\n",
    "        url = link.get(\"url\")\n",
    "        if url:\n",
    "            urls.append(url)\n",
    "\n",
    "# Remove duplicates\n",
    "urls = list(set(urls))\n",
    "\n",
    "print(f\"Found {len(urls)} URLs:\")\n",
    "print(urls)\n",
    "\n",
    "\n",
    "# Use https://codebeautify.org/base64-to-image-converter to view the base64 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64d9f175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found 3 tables.\n",
      "\n",
      "--- Table 1 ---\n",
      "Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2 Â· d) O(1) O(1) Recurrent O(n Â· d2) O(n) O(n) Convolutional O(k Â· n Â· d2) O(1) O(logk(n)) Self-Attention (restricted) O(r Â· n Â· d) O(1) O(n/r)\n",
      "\n",
      "--- Table 2 ---\n",
      "Model BLEU EN-DE EN-FR Training Cost (FLOPs) EN-DE EN-FR ByteNet [15] 23.75 Deep-Att + PosUnk [32] 39.2 1.0 Â· 1020 GNMT + RL [31] 24.6 39.92 2.3 Â· 1019 1.4 Â· 1020 ConvS2S [8] 25.16 40.46 9.6 Â· 1018 1.5 Â· 1020 MoE [26] 26.03 40.56 2.0 Â· 1019 1.2 Â· 1020 Deep-Att + PosUnk Ensemble [32] 40.4 8.0 Â· 1020 GNMT + RL Ensemble [31] 26.30 41.16 1.8 Â· 1020 1.1 Â· 1021 ConvS2S Ensemble [8] 26.36 41.29 7.7 Â· 1019 1.2 Â· 1021 Transformer (base model) 27.3 38.1 3.3 Â· 1018 Transformer (big) 28.4 41.0 2.3 Â· 1019\n",
      "\n",
      "--- Table 3 ---\n",
      "N odel [ dy dy â‚¬ls train PPL BLEU params Pyrop steps (dev) (dev) 100 base 6 512 2048 64 64 0.1 0.1 100K 4.92 25.8 65 512 512 5.29 24.9 128 128 5.00 25.5 (A) 32 32 491 25.8 B o > 16 16 5.01 25.4 (B) 16 5.16 25.1 58 32 5.01 25.4 60 6.11 23.7 36 5.19 25.3 50 o0 B 4.88 25.5 80 256 32 32 5.75 24.5 28 Â© 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 0.2 4.95 25.5 D) 0.0 4.67 25.3 0.2 5.47 25.7 Â® 4.92 25.7 positional embedding instead of sinusoids big 1024 4096 16 0.3 300K 4.33 264 213\n"
     ]
    }
   ],
   "source": [
    "# Gather all table\n",
    "alltables = []\n",
    "\n",
    "for el in elements:\n",
    "    # Check if the element has a type and it equals \"Table\"\n",
    "    if el.get(\"type\") == \"Table\":\n",
    "        alltables.append(el)\n",
    "\n",
    "# Optional: print how many tables were found\n",
    "print(f\"âœ… Found {len(alltables)} tables.\")\n",
    "\n",
    "# Optional: print a preview\n",
    "for i, table in enumerate(alltables[:3], start=1):  # show first 3 tables\n",
    "    print(f\"\\n--- Table {i} ---\")\n",
    "    print(table[\"text\"])\n",
    "\n",
    "# Use https://jsfiddle.net/ to view the table html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841876e",
   "metadata": {},
   "source": [
    "### Chunk local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90411d5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
