{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37178c8f",
   "metadata": {},
   "source": [
    "## ***üß© Element Selection Strategy for Unstructured Documents Chunk Clubing***\n",
    "\n",
    "| **Element** | **Type (to maintain in chunk)** | **Importance** | **Reason / Description** | **Action to Take** |\n",
    "|--------------|--------------------------------|----------------|---------------------------|--------------------|\n",
    "| **Text** | `Text` | ‚úÖ **Highly Important** | Contains main document content, including paragraphs, inline formulas, and contextual information. | Always include during extraction and RAG processing. |\n",
    "| **Table** | `Table` | ‚úÖ **Highly Important** | Holds structured data such as metrics, comparisons, and datasets. | Always retain and preserve cell structure if possible. |\n",
    "| **Image + FigureCaption** | `Image+Caption` | ‚úÖ **Important (Combined)** | Images provide visual info; FigureCaptions describe the image context. | Combine both ‚Äî keep the image and attach the caption as description or metadata. |\n",
    "| **Formula** | `Formula` | ‚öôÔ∏è **Not Needed Separately** | Formulas are often embedded inline within text; separate extraction is redundant. | Skip separate extraction ‚Äî rely on text content. |\n",
    "| **ListItem** | `ListItem` | ‚öôÔ∏è **Not Needed** | Lists are already represented within text blocks. | Exclude individual list items. |\n",
    "| **NarrativeText** | `NarrativeText` | ‚öôÔ∏è **Not Needed** | Narrative text overlaps with the main text content. | Do not extract separately. |\n",
    "| **Footer** | `Footer` | ‚úÖ **Very Important** | Often includes metadata like page numbers, document versions, and timestamps. | Extract and store separately when available. |\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ **Conclusion**\n",
    "\n",
    "| **Keep / Exclude** | **Elements** | **Type to Maintain** | **Notes** |\n",
    "|---------------------|--------------|----------------------|------------|\n",
    "| ‚úÖ **Keep** | **Text**, **Table**, **Image + FigureCaption (combined)**, **Footer** | `Text`, `Table`, `Image+Caption`, `Footer` | These carry the most relevant and non-redundant information. |\n",
    "| ‚ùå **Exclude** | **Formula**, **ListItem**, **NarrativeText** | `Formula`, `ListItem`, `NarrativeText` | These are redundant or already captured within text content. |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ **Final Recommendation**\n",
    "> Focus on the following elements for your RAG or document extraction pipeline:\n",
    "> - **Text** ‚Üí Type: `Text`\n",
    "> - **Table** ‚Üí Type: `Table`\n",
    "> - **Image + FigureCaption (combined)** ‚Üí Type: `Image+Caption`\n",
    "> - **Footer** ‚Üí Type: `Footer`\n",
    ">\n",
    "> Maintain the **type field** in each chunk so you always know what kind of content it contains.  \n",
    "> This improves traceability, retrieval accuracy, and contextual organization across your RAG workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2238bccb",
   "metadata": {},
   "source": [
    "### ***Imports Required***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Unstructured for document parsing\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "from unstructured.documents.elements import Element\n",
    "\n",
    "# LangChain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c68589",
   "metadata": {},
   "source": [
    "### ***Partion of documents***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bf16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "def partition_document_launcher(\n",
    "    file_path: str,\n",
    "    max_characters: int,\n",
    "    new_after_n_chars: int,\n",
    "    combine_text_under_n_chars: int,\n",
    "    extract_images: bool = False,\n",
    "    extract_tables: bool = False,\n",
    "    languages: List[str] = ['eng']\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract elements from PDF using unstructured library.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the PDF file to process (REQUIRED)\n",
    "        max_characters: Maximum characters per chunk (REQUIRED)\n",
    "        new_after_n_chars: Start new chunk after this many characters (REQUIRED)\n",
    "        combine_text_under_n_chars: Combine small text blocks under this count (REQUIRED)\n",
    "        extract_images: Whether to extract images from the PDF 'True' or 'False'\n",
    "        extract_tables: Whether to infer table structure 'True' or 'False'\n",
    "        languages: List of language codes (defaults to ['eng'])\n",
    "    \n",
    "    Returns:\n",
    "        List of extracted elements from the PDF\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the PDF file doesn't exist\n",
    "        ValueError: If invalid parameters are provided\n",
    "    \"\"\"\n",
    "    # Validate input file\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"PDF file not found: {file_path}\")\n",
    "    \n",
    "    # Validate chunk parameters\n",
    "    if max_characters >= new_after_n_chars:\n",
    "        raise ValueError(\"max_characters must be less than new_after_n_chars\")\n",
    "    \n",
    "    # Set image output directory (fixed path)\n",
    "    image_output_dir = r\"D:\\MultiModulRag\\Backend\\SmartPipelinedef\\Images\"\n",
    "    \n",
    "    # Create image directory if extracting images\n",
    "    if extract_images:\n",
    "        Path(image_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"üìÑ Partitioning document: {file_path}\")\n",
    "    print(f\"‚öôÔ∏è  Settings: Images={extract_images}, Tables={extract_tables}, Languages={languages}\")\n",
    "    print(f\"üìä Chunk settings: max={max_characters}, new_after={new_after_n_chars}, combine={combine_text_under_n_chars}\")\n",
    "\n",
    "    elements = partition_pdf(\n",
    "        ### File path is always require and important ###\n",
    "        filename=file_path,\n",
    "\n",
    "        ### Core parameters (Fixed Parameters) ###\n",
    "        strategy=\"hi_res\",\n",
    "        hi_res_model_name=\"yolox\",\n",
    "        chunking_strategy=\"by_title\",\n",
    "        include_orig_elements=True,\n",
    "\n",
    "        ### Language and extraction parameters ###\n",
    "        languages=languages,  # Use the parameter instead of empty list\n",
    "        \n",
    "        ### Image extraction parameters ###\n",
    "        extract_images_in_pdf=extract_images,\n",
    "        extract_image_block_to_payload=extract_images,\n",
    "        extract_image_block_output_dir=image_output_dir if extract_images else None,\n",
    "        extract_image_block_types=[\"Image\"] if extract_images else [],\n",
    "        \n",
    "        ### Table extraction ###\n",
    "        infer_table_structure=extract_tables,  # Use the parameter\n",
    "        \n",
    "        ### Chunk parameters ###\n",
    "        max_characters=max_characters,\n",
    "        new_after_n_chars=new_after_n_chars,\n",
    "        combine_text_under_n_chars=combine_text_under_n_chars,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(elements)} elements\")\n",
    "    \n",
    "    # Print element breakdown\n",
    "    element_types = {}\n",
    "    for elem in elements:\n",
    "        elem_type = type(elem).__name__\n",
    "        element_types[elem_type] = element_types.get(elem_type, 0) + 1\n",
    "    print(f\"üìã Element breakdown: {dict(element_types)}\")\n",
    "    \n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43207c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Partitioning document: D:\\MultiModulRag\\docs\\NIPS-2017-attention-is-all-you-need-Paper_removed.pdf\n",
      "‚öôÔ∏è  Settings: Images=True, Tables=True, Languages=['eng']\n",
      "üìä Chunk settings: max=3000, new_after=3800, combine=200\n",
      "‚úÖ Extracted 4 elements\n",
      "üìã Element breakdown: {'CompositeElement': 4}\n"
     ]
    }
   ],
   "source": [
    "checkpoint1 = partition_document_launcher (file_path =r\"D:\\MultiModulRag\\docs\\NIPS-2017-attention-is-all-you-need-Paper_removed.pdf\",\n",
    "                                          max_characters=3000,\n",
    "                                          new_after_n_chars=3800,\n",
    "                                          combine_text_under_n_chars=200,\n",
    "                                          extract_images=True,\n",
    "                                          extract_tables=True,\n",
    "                                          languages=['eng'],            \n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c00ff35",
   "metadata": {},
   "source": [
    "### ***Checkpoint1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bff0f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved elements to pickle: D:\\MultiModulRag\\Backend\\SmartPipelinedef\\Pickel\\checkpoint1.pkl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from unstructured.documents.elements import Element\n",
    "\n",
    "def save_elements(elements, pkl_path: str, json_path: str = None):\n",
    "    \"\"\"\n",
    "    Save a Python variable `elements` to pickle and optionally to JSON.\n",
    "    Automatically converts unstructured Element objects to dicts for JSON.\n",
    "\n",
    "    Args:\n",
    "        elements: Python variable to save (list, dict, etc.)\n",
    "        pkl_path: Path to save the pickle file (required)\n",
    "    \"\"\"\n",
    "    # Ensure parent directories exist\n",
    "    Path(pkl_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    if json_path:\n",
    "        Path(json_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save as Pickle\n",
    "    with open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(elements, f)\n",
    "    print(f\"‚úÖ Saved elements to pickle: {pkl_path}\")\n",
    "\n",
    "    # # Save as JSON (optional)\n",
    "    # if json_path:\n",
    "    #     # Convert Element objects to dicts automatically\n",
    "    #     def to_serializable(el):\n",
    "    #         return el.to_dict() if isinstance(el, Element) else el\n",
    "        \n",
    "    #     elements_serializable = [to_serializable(el) for el in elements]\n",
    "\n",
    "    #     with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    #         json.dump(elements_serializable, f, indent=4, ensure_ascii=False)\n",
    "    #     print(f\"‚úÖ Saved elements to JSON: {json_path}\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# your Python variable, e.g., output of partition_pdf\n",
    "\n",
    "pkl_file = r\"D:\\MultiModulRag\\Backend\\SmartPipelinedef\\Pickel\\checkpoint1.pkl\"\n",
    "\n",
    "save_elements(checkpoint1, pkl_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83098f75",
   "metadata": {},
   "source": [
    "### ***Loding Checkpoint1 Pickel***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ed11502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Load Pickel has : 4 elements\n"
     ]
    }
   ],
   "source": [
    "pkl_path = r\"D:\\MultiModulRag\\Backend\\SmartPipelinedef\\Pickel\\checkpoint1.pkl\"\n",
    "with open(pkl_path, \"rb\") as f:\n",
    "        loaded1 = pickle.load(f)\n",
    "print(f\"‚úÖ Load Pickel has : {len(loaded1)} elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d05be8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Image at 0x19013d75310>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x19013d75c50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x19013d75e10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x19013d761d0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded1[0].metadata.orig_elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc505f9",
   "metadata": {},
   "source": [
    "### ***Combing Content & Genrating AI Embeddings***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e50f7d",
   "metadata": {},
   "source": [
    "##### ***Clean Image Directory Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6205c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_image_directory(image_dir: str) -> None:\n",
    "    \"\"\"Clean existing images from directory\"\"\"\n",
    "    Path(image_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for file in Path(image_dir).glob(\"*\"):\n",
    "        if file.is_file():\n",
    "            try:\n",
    "                file.unlink()\n",
    "                print(f\"     üóëÔ∏è  Deleted old image: {file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"     ‚ö†Ô∏è  Could not delete {file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1cab4",
   "metadata": {},
   "source": [
    "##### ***Separate content types & Make an Image from Image64 formate and save it in directory***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66d57d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "\n",
    "def separate_content_types(chunk, image_dir: str, image_counter: dict) -> dict[str, any]:\n",
    "    \"\"\"\n",
    "    Analyze chunk content and extract text, tables, and images.\n",
    "    Uses mutable dict to track image counter across calls.\n",
    "    \"\"\"\n",
    "    content_data = {\n",
    "        'text': chunk.text,\n",
    "        'tables': [],\n",
    "        'image_base64': [],\n",
    "        'images_dirpath': [],  # now will hold folder + filename\n",
    "        'page_no': [],\n",
    "        'types': ['text']\n",
    "    }\n",
    "\n",
    "    for element in chunk.metadata.orig_elements:\n",
    "        element_type = type(element).__name__\n",
    "\n",
    "        # Handle page numbers\n",
    "        if 'metadata' in element.to_dict():\n",
    "            page_no = element.to_dict()['metadata'].get('page_number')\n",
    "            if page_no and page_no not in content_data['page_no']:\n",
    "                content_data['page_no'].append(page_no)\n",
    "\n",
    "        # Handle tables\n",
    "        if element_type == 'Table':\n",
    "            if 'table' not in content_data['types']:\n",
    "                content_data['types'].append('table')\n",
    "            table_html = getattr(element.metadata, 'text_as_html', element.text)\n",
    "            content_data['tables'].append(table_html)\n",
    "\n",
    "        # Handle images\n",
    "        elif element_type == 'Image':\n",
    "            if hasattr(element.metadata, 'image_base64'):\n",
    "                if 'image' not in content_data['types']:\n",
    "                    content_data['types'].append('image')\n",
    "\n",
    "                image_base64 = element.metadata.image_base64\n",
    "\n",
    "                try:\n",
    "                    # Generate filename and path\n",
    "                    image_filename = f\"image_{image_counter['count']:04d}.png\"\n",
    "                    image_path = os.path.join(image_dir, image_filename)\n",
    "\n",
    "                    # Decode and save image\n",
    "                    with open(image_path, \"wb\") as img_file:\n",
    "                        img_file.write(base64.b64decode(image_base64))\n",
    "\n",
    "                    # ‚úÖ Store relative folder + filename (e.g. \"Images/image_0001.png\")\n",
    "                    folder_name = os.path.basename(image_dir.rstrip(os.sep))\n",
    "                    relative_path = os.path.join(folder_name, image_filename).replace(\"\\\\\", \"/\")\n",
    "                    content_data['images_dirpath'].append(relative_path)\n",
    "\n",
    "                    # Keep base64 for AI processing\n",
    "                    content_data['image_base64'].append(image_base64)\n",
    "\n",
    "                    print(f\"     ‚úÖ Saved: {relative_path}\")\n",
    "                    image_counter['count'] += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"     ‚ùå Failed to save image {image_counter['count']}: {e}\")\n",
    "\n",
    "    return content_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb485a",
   "metadata": {},
   "source": [
    "##### ***Creating AI Summary for embeddings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7df7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ai_enhanced_summary(text: str, tables: list[str], images: list[str]) -> str:\n",
    "    \"\"\"Create AI-enhanced summary for mixed content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0)\n",
    "        \n",
    "        # Build comprehensive prompt\n",
    "        prompt_text = f\"\"\"You are creating a searchable description for document content retrieval.\n",
    "\n",
    "CONTENT TO ANALYZE:\n",
    "\n",
    "TEXT CONTENT:\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Add tables if present\n",
    "        if tables:\n",
    "            prompt_text += \"TABLES:\\n\"\n",
    "            for i, table in enumerate(tables, 1):\n",
    "                prompt_text += f\"Table {i}:\\n{table}\\n\\n\"\n",
    "        \n",
    "        # Add detailed instructions\n",
    "        prompt_text += \"\"\"\n",
    "YOUR TASK:\n",
    "Generate a comprehensive, searchable description that covers:\n",
    "\n",
    "1. Key facts, numbers, and data points from text and tables\n",
    "2. Main topics and concepts discussed  \n",
    "3. Questions this content could answer\n",
    "4. Visual content analysis (charts, diagrams, patterns in images)\n",
    "5. Alternative search terms users might use\n",
    "\n",
    "Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "QUESTIONS: \"List all potential questions that can be answered from this content (text, images, tables)\"\n",
    "SUMMARY: \"Comprehensive summary of all data and information\"\n",
    "IMAGE_INTERPRETATION: \"Detailed description of image content. If images are irrelevant or contain only decorative elements, state: ***DO NOT USE THIS IMAGE***\"\n",
    "TABLE_INTERPRETATION: \"Detailed description of table content. If tables are irrelevant, state: ***DO NOT USE THIS TABLE***\"\n",
    "\n",
    "SEARCHABLE DESCRIPTION:\"\"\"\n",
    "\n",
    "        # Build message with text and images\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add images to message\n",
    "        for img_b64 in images:\n",
    "            message_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{img_b64}\"}\n",
    "            })\n",
    "        \n",
    "        # Invoke AI\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ‚ùå AI summary failed: {e}\")\n",
    "        # Fallback summary\n",
    "        summary = f\"{text[:300]}...\"\n",
    "        if tables:\n",
    "            summary += f\"\\n[Contains {len(tables)} table(s)]\"\n",
    "        if images:\n",
    "            summary += f\"\\n[Contains {len(images)} image(s)]\"\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b798a54d",
   "metadata": {},
   "source": [
    "##### ***Make united pipeline***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ee133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarise_chunks(chunks, image_dir: str = r\"D:\\MultiModulRag\\Backend\\SmartPipelinedef\\Images\") -> list[Document]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Process all chunks with AI Summaries.\n",
    "\n",
    "    Args:\n",
    "        chunks: List of document chunks to process\n",
    "        image_dir: Directory to store extracted images\n",
    "        \n",
    "    Returns:\n",
    "        List of LangChain Documents with enhanced summaries\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üß† Processing chunks with AI Summaries...\")\n",
    "    \n",
    "    # Clean image directory once\n",
    "    clean_image_directory(image_dir)\n",
    "    \n",
    "    langchain_documents = []\n",
    "    total_chunks = len(chunks)\n",
    "    image_counter = {'count': 1}  # Use mutable dict to pass by reference\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"\\n   üìÑ Processing chunk {i}/{total_chunks}\")\n",
    "        \n",
    "        # Analyze chunk content\n",
    "        content_data = separate_content_types(chunk, image_dir, image_counter)\n",
    "        \n",
    "        # Debug info\n",
    "        print(f\"     Types found: {', '.join(content_data['types'])}\")\n",
    "        print(f\"     Tables: {len(content_data['tables'])}, Images: {len(content_data['image_base64'])}\")\n",
    "        if content_data['page_no']:\n",
    "            print(f\"     Pages: {content_data['page_no']}\")\n",
    "        \n",
    "        # Create AI-enhanced summary for ALL chunks\n",
    "        print(f\"      Creating AI summary...\")\n",
    "        try:\n",
    "            enhanced_content = create_ai_enhanced_summary(\n",
    "                content_data['text'],\n",
    "                content_data['tables'], \n",
    "                content_data['image_base64']\n",
    "            )\n",
    "            print(f\"      AI summary created\")\n",
    "            print(f\"     Preview: {enhanced_content[:150]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"      AI summary failed, using raw text: {e}\")\n",
    "            enhanced_content = content_data['text']\n",
    "        \n",
    "        # Create LangChain Document with metadata\n",
    "        # Store image paths instead of base64 to reduce memory usage\n",
    "        doc = Document(\n",
    "            page_content=enhanced_content,\n",
    "                # 'text': chunk.text,\n",
    "                # 'tables': [],\n",
    "                # 'images_base64': [],\n",
    "                # 'images_dirpath': [],\n",
    "                # 'page_no': [],\n",
    "                # 'types': ['text']\n",
    "            metadata={\n",
    "                \"chunk_index\": i,\n",
    "                \"original_text\": content_data['text'],\n",
    "                \"raw_tables_html\": content_data['tables'],\n",
    "                \"image_paths\": content_data['images_dirpath'],\n",
    "                \"page_numbers\": content_data['page_no'],\n",
    "                \"content_types\": content_data['types'],\n",
    "                # \"num_tables\": len(content_data['tables']),\n",
    "                # \"num_images\": len(content_data['images_dirpath']),\n",
    "                # \"original_content\": json.dumps({\n",
    "                # \"raw_text\": content_data['text'],\n",
    "                # Don't store base64 in metadata to save space\n",
    "                # \"has_images\": len(content_data['images_base64']) > 0\n",
    "                # })\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        langchain_documents.append(doc)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully processed {len(langchain_documents)} chunks\")\n",
    "    print(f\"üìä Total images saved: {image_counter['count'] - 1}\")\n",
    "    \n",
    "    return langchain_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8dd742",
   "metadata": {},
   "source": [
    "##### ***Run pipeline***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3cc0a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Processing chunks with AI Summaries...\n",
      "\n",
      "   üìÑ Processing chunk 1/4\n",
      "     ‚úÖ Saved: Images/image_0001.png\n",
      "     Types found: text, image\n",
      "     Tables: 0, Images: 1\n",
      "     Pages: [1]\n",
      "      Creating AI summary...\n",
      "      AI summary created\n",
      "     Preview: QUESTIONS:\n",
      "*   What is the model architecture of the Transformer?\n",
      "*   What are the main components of the Transformer model?\n",
      "*   How is the Transforme...\n",
      "\n",
      "   üìÑ Processing chunk 2/4\n",
      "     Types found: text\n",
      "     Tables: 0, Images: 0\n",
      "     Pages: [1]\n",
      "      Creating AI summary...\n",
      "      AI summary created\n",
      "     Preview: **QUESTIONS:**\n",
      "\"What is an attention function?\n",
      "How can an attention function be described?\n",
      "What are the inputs and outputs of an attention function?\n",
      "W...\n",
      "\n",
      "   üìÑ Processing chunk 3/4\n",
      "     ‚úÖ Saved: Images/image_0002.png\n",
      "     ‚úÖ Saved: Images/image_0003.png\n",
      "     Types found: text, image\n",
      "     Tables: 0, Images: 2\n",
      "     Pages: [1, 2]\n",
      "      Creating AI summary...\n",
      "      AI summary created\n",
      "     Preview: **QUESTIONS:**\n",
      "*   What is Scaled Dot-Product Attention?\n",
      "*   What is the formula for Scaled Dot-Product Attention?\n",
      "*   What are the inputs to the Scal...\n",
      "\n",
      "   üìÑ Processing chunk 4/4\n",
      "     Types found: text\n",
      "     Tables: 0, Images: 0\n",
      "     Pages: [2]\n",
      "      Creating AI summary...\n",
      "      AI summary created\n",
      "     Preview: **QUESTIONS:**\n",
      "\"What is Multi-Head Attention?\n",
      "How does Multi-Head Attention work?\n",
      "What is the benefit of using Multi-Head Attention over a single atte...\n",
      "\n",
      "‚úÖ Successfully processed 4 chunks\n",
      "üìä Total images saved: 3\n"
     ]
    }
   ],
   "source": [
    "output = summarise_chunks(loaded1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d53bd11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**QUESTIONS:**\\n\"What is Multi-Head Attention?\\nHow does Multi-Head Attention work?\\nWhat is the benefit of using Multi-Head Attention over a single attention function?\\nHow are queries, keys, and values processed in Multi-Head Attention?\\nWhat are the dimensions dk, dv, and dmodel in the context of attention?\\nHow many times are the queries, keys, and values projected?\\nWhat happens to the outputs of the parallel attention functions?\\nHow does Multi-Head Attention allow a model to attend to different representation subspaces?\\nWhy does single-head attention inhibit attending to different subspaces?\\nWhat is the statistical explanation for why dot products get large in attention mechanisms?\\nAssuming query (q) and key (k) components are independent random variables with mean 0 and variance 1, what is the mean and variance of their dot product?\\nWhat is the variance of the dot product q ¬∑ k?\\nWhat process is depicted in Figure 2?\"\\n\\n**SUMMARY:**\\n\"This section, 3.2.2, describes the Multi-Head Attention mechanism. Instead of using a single attention function with dmodel-dimensional queries, keys, and values, this approach uses multiple parallel attention functions, or \\'heads\\'. Specifically, the queries, keys, and values are linearly projected \\'h\\' times into lower dimensions: dk, dk, and dv, respectively. These projections are learned and different for each head.\\n\\nThe attention function is then performed in parallel on each of these \\'h\\' projected sets of queries, keys, and values. Each head produces a dv-dimensional output. These \\'h\\' output vectors are then concatenated together and passed through another linear projection to produce the final output values.\\n\\nThe primary benefit of this method is that it allows the model to jointly attend to information from different representation subspaces at different positions simultaneously. In contrast, a single attention head would average these different pieces of information, which can inhibit this capability.\\n\\nThe text also provides a statistical justification for scaling dot products. It illustrates that if the components of the query (q) and key (k) vectors are assumed to be independent random variables with a mean of 0 and a variance of 1, then their dot product (q ¬∑ k) will have a mean of 0 and a variance equal to the dimension of the key, dk. This increasing variance with dimensionality is why scaling is necessary.\"\\n\\n**IMAGE_INTERPRETATION:**\\n\"The text references Figure 2, which is not provided. Based on the description, Figure 2 would be a diagram illustrating the architecture of Multi-Head Attention. It would likely show:\\n1.  Inputs for Queries (Q), Keys (K), and Values (V).\\n2.  These inputs being fed into \\'h\\' parallel blocks.\\n3.  Each block containing a linear projection layer for Q, K, and V, followed by a \\'Scaled Dot-Product Attention\\' unit.\\n4.  The \\'h\\' output vectors from these parallel attention units.\\n5.  A \\'Concat\\' operation that combines these \\'h\\' output vectors.\\n6.  A final \\'Linear\\' projection layer that takes the concatenated vector as input and produces the final output of the Multi-Head Attention block.\"\\n\\n**TABLE_INTERPRETATION:**\\n\"***DO NOT USE THIS TABLE***\"\\n\\n**SEARCHABLE DESCRIPTION:**\\n\"Multi-Head Attention, Section 3.2.2, Attention mechanism, Transformer architecture, Self-attention, Queries, Keys, Values, QKV, Linear projection, Parallel attention functions, h heads, dk dimension, dv dimension, dmodel dimension, Concatenation of outputs, Representation subspaces, Jointly attend to information, Single-head attention, Averaging inhibits attention, Dot product, Dot product variance, Statistical properties of attention, Independent random variables, Mean 0, Variance 1, Variance of dot product is dk, Figure 2, Model architecture, Neural networks, Deep learning.\"'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[3].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81797ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Images/image_0001.png']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].metadata['image_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf709e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Images in directory: 3\n",
      "   - image_0001.png: 71463 bytes\n",
      "   - image_0002.png: 12578 bytes\n",
      "   - image_0003.png: 20672 bytes\n"
     ]
    }
   ],
   "source": [
    "# Check if images were actually saved\n",
    "image_dir = r\"D:\\MultiModulRag\\Backend\\SmartPipelinedef\\Images\"\n",
    "saved_images = list(Path(image_dir).glob(\"*.png\"))\n",
    "print(f\"\\nüìÅ Images in directory: {len(saved_images)}\")\n",
    "for img in saved_images:\n",
    "    print(f\"   - {img.name}: {img.stat().st_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4a4a1",
   "metadata": {},
   "source": [
    "### ***Checkpointing Output Pickel & JSON***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6626ade1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pickle saved: D:\\MultiModulRag\\Backend\\SmartPipelinedef\\Pickel\\output.pkl\n",
      "‚úÖ JSON saved: D:\\MultiModulRag\\Backend\\SmartPipelinedef\\JSON\\output.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, pickle\n",
    "\n",
    "pkl_path = r\"D:\\MultiModulRag\\Backend\\SmartPipelinedef\\Pickel\\output.pkl\"\n",
    "json_path = r\"D:\\MultiModulRag\\Backend\\SmartPipelinedef\\JSON\\output.json\"\n",
    "\n",
    "os.makedirs(os.path.dirname(pkl_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "\n",
    "# üíæ Save Pickle\n",
    "with open(pkl_path, \"wb\") as f:\n",
    "    pickle.dump(output, f)\n",
    "print(f\"‚úÖ Pickle saved: {pkl_path}\")\n",
    "\n",
    "# üß© Convert to clean JSON format (including enhanced content)\n",
    "clean_json = [\n",
    "    {\n",
    "        \"chunk_index\": doc.metadata.get(\"chunk_index\"),\n",
    "        \"enhanced_content\": getattr(doc, \"page_content\", \"\"),  # from Document\n",
    "        \"original_text\": doc.metadata.get(\"original_text\", \"\"),\n",
    "        \"raw_tables_html\": doc.metadata.get(\"raw_tables_html\", []),\n",
    "        \"image_paths\": doc.metadata.get(\"image_paths\", []),\n",
    "        \"page_numbers\": doc.metadata.get(\"page_numbers\", []),\n",
    "        \"content_types\": doc.metadata.get(\"content_types\", []),\n",
    "    }\n",
    "    for doc in output\n",
    "]\n",
    "\n",
    "# üíæ Save JSON\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(clean_json, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ JSON saved: {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296310f9",
   "metadata": {},
   "source": [
    "### ***Loading Output Pickel***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1665632a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ AI content produced: QUESTIONS:\n",
      "*   What is the model architecture of the Transformer?\n",
      "*   What are the main components of the Transformer model?\n",
      "*   How is the Transformer's encoder structured?\n",
      "*   How is the Transformer...\n",
      "üìä Metadata: {'chunk_index': 1, 'original_text': 'Output Probabilities Add & Norm Feed Forward Add & Norm Multi-Head Attention a, Add & Norm Add & Norm Feed Forward Nx | -+CAgc8 Norm) Add & Norm Masked Multi-Head Multi-Head Attention Attention Se a, ee a, Positional Positional Encoding @ ¬© @ Encoding Input Output Embedding Embedding Inputs Outputs (shifted right)\\n\\nFigure 1: The Transformer - model architecture.\\n\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.', 'raw_tables_html': [], 'image_paths': ['Images/image_0001.png'], 'page_numbers': [1], 'content_types': ['text', 'image']}\n"
     ]
    }
   ],
   "source": [
    "pkl_dir = r\"D:\\MultiModulRag\\Backend\\SmartPipelinedef\\Pickel\\output.pkl\"\n",
    "with open(pkl_dir, 'rb') as f:\n",
    "    loaded_docs = pickle.load(f)\n",
    "\n",
    "# Now this will work:\n",
    "print(f\"üìÑ AI content produced: {loaded_docs[0].page_content[:200]}...\")\n",
    "print(f\"üìä Metadata: {loaded_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad16ebb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_index': 1,\n",
       " 'original_text': 'Output Probabilities Add & Norm Feed Forward Add & Norm Multi-Head Attention a, Add & Norm Add & Norm Feed Forward Nx | -+CAgc8 Norm) Add & Norm Masked Multi-Head Multi-Head Attention Attention Se a, ee a, Positional Positional Encoding @ ¬© @ Encoding Input Output Embedding Embedding Inputs Outputs (shifted right)\\n\\nFigure 1: The Transformer - model architecture.\\n\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\\n\\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.',\n",
       " 'raw_tables_html': [],\n",
       " 'image_paths': ['Images/image_0001.png'],\n",
       " 'page_numbers': [1],\n",
       " 'content_types': ['text', 'image']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60cca9",
   "metadata": {},
   "source": [
    "### ***Into Store in VectorDB***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d35f724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÆ Creating embeddings and storing in ChromaDB...\n",
      "--- Creating vector store ---\n",
      "--- Finished creating vector store ---\n",
      "‚úÖ Vector store created and saved to D:\\MultiModulRag\\Backend\\SmartPipelinedef\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def create_vector_store(documents, persist_directory=r\"D:\\MultiModulRag\\Backend\\SmartPipelinedef\\chroma_db\"):\n",
    "    \"\"\"Create and persist ChromaDB vector store\"\"\"\n",
    "    print(\"üîÆ Creating embeddings and storing in ChromaDB...\")\n",
    "    \n",
    "    # Convert list metadata to JSON strings\n",
    "    for doc in documents:\n",
    "        if \"raw_tables_html\" in doc.metadata:\n",
    "            doc.metadata[\"raw_tables_html\"] = json.dumps(doc.metadata[\"raw_tables_html\"])\n",
    "        if \"image_paths\" in doc.metadata:\n",
    "            doc.metadata[\"image_paths\"] = json.dumps(doc.metadata[\"image_paths\"])\n",
    "        if \"page_numbers\" in doc.metadata:\n",
    "            doc.metadata[\"page_numbers\"] = json.dumps(doc.metadata[\"page_numbers\"])\n",
    "        if \"content_types\" in doc.metadata:\n",
    "            doc.metadata[\"content_types\"] = json.dumps(doc.metadata[\"content_types\"])\n",
    "    \n",
    "    embedding_model = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\")\n",
    "    \n",
    "    print(\"--- Creating vector store ---\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(\"--- Finished creating vector store ---\")\n",
    "    \n",
    "    print(f\"‚úÖ Vector store created and saved to {persist_directory}\")\n",
    "    return vectorstore\n",
    "\n",
    "# Create the vector store\n",
    "db = create_vector_store(loaded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e3327",
   "metadata": {},
   "source": [
    "### ***Retrival from VDB***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7412f3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading vector store from: D:\\MultiModulRag\\Backend\\Pipeline_Database\\chroma_db\n",
      "üîç Searching for: What is this pdf about and how to learn it?\n",
      "\n",
      "üìÑ Chunk 1:\n",
      "   Pages: [1, 2]\n",
      "   Types: [\"text\", \"image\"]\n",
      "   üìä Tables: 2\n",
      "   üñºÔ∏è Images: 146\n",
      "\n",
      "üìÑ Chunk 2:\n",
      "   Pages: [1]\n",
      "   Types: [\"text\", \"image\"]\n",
      "   üìä Tables: 2\n",
      "   üñºÔ∏è Images: 73\n",
      "\n",
      "ü§ñ Generating answer...\n",
      "\n",
      "üí° Answer:\n",
      "Based on the provided context, here is an answer to your question.\n",
      "\n",
      "### What is this PDF about?\n",
      "\n",
      "This document describes the **Transformer model architecture**, a highly influential neural network design primarily used for natural language processing (NLP) and other sequence-to-sequence tasks. The paper it originates from is titled \"Attention Is All You Need.\"\n",
      "\n",
      "Based on the context, the key concepts covered are:\n",
      "\n",
      "1.  **The Core Mechanism: Scaled Dot-Product Attention:** This is the fundamental building block of the Transformer. It's a mechanism that allows the model to weigh the importance of different words (or parts of a sequence) when processing a particular word.\n",
      "    *   It operates on a set of **queries (Q)**, **keys (K)**, and **values (V)**.\n",
      "    *   The attention weights are calculated using the formula: `Attention(Q,K,V) = softmax( (Q * K^T) / ‚àödk ) * V`.\n",
      "    *   The scaling factor (`1/‚àödk`) is crucial for stabilizing the training process, especially when the dimensions (`dk`) are large, by preventing the softmax function from having extremely small gradients.\n",
      "\n",
      "2.  **Multi-Head Attention:** This is an enhancement to the basic attention mechanism. Instead of performing attention once, it runs multiple \"Scaled Dot-Product Attention\" operations in parallel (the \"heads\"). This allows the model to jointly attend to information from different representational subspaces at different positions, capturing a richer variety of relationships.\n",
      "\n",
      "3.  **The Overall Architecture:** The document outlines the full encoder-decoder structure of the Transformer model.\n",
      "    *   **Encoder and Decoder Stacks:** The model is composed of a stack of encoders and a stack of decoders (typically N=6 of each).\n",
      "    *   **Sub-layers:** Each layer contains sub-layers, including Multi-Head Attention and a Feed-Forward Network.\n",
      "    *   **Residual Connections and Layer Normalization:** To improve training stability and information flow, the model uses residual connections (`x + Sublayer(x)`) followed by layer normalization (`LayerNorm(...)`) around each sub-layer.\n",
      "    *   **Decoder-Specific Attention:** The decoder has a special \"masked\" multi-head self-attention sub-layer to ensure that when predicting a word, it can only attend to previous words in the sequence. It also has a third sub-layer that performs attention over the encoder's output, allowing it to focus on relevant parts of the input sequence.\n",
      "\n",
      "---\n",
      "\n",
      "### How to learn it?\n",
      "\n",
      "Learning the Transformer architecture can be broken down into a structured, step-by-step process.\n",
      "\n",
      "**Step 1: Master the Prerequisites**\n",
      "Before tackling the Transformer, ensure you have a solid understanding of:\n",
      "*   **Linear Algebra:** Especially matrix multiplication, as it is the core operation in the attention formula.\n",
      "*   **Neural Network Fundamentals:** Understand concepts like layers, weights, activation functions (especially softmax), and backpropagation/gradients.\n",
      "*   **Sequence Models (for context):** Learn about Recurrent Neural Networks (RNNs) and LSTMs. Understanding their limitations (e.g., difficulty with long-range dependencies, sequential computation) will clarify *why* the Transformer was a breakthrough.\n",
      "\n",
      "**Step 2: Understand the Core Concept: Attention**\n",
      "*   **Intuition:** Start with the high-level idea of attention. Think of it as a search or a lookup. For a given **query** (e.g., the word we're currently processing), we compare it against a set of **keys** (e.g., all other words in the sentence) to find how relevant they are. We then use these relevance scores to create a weighted sum of the **values** (the information from those other words).\n",
      "*   **Scaled Dot-Product Attention:** Carefully walk through the formula `softmax( (Q * K^T) / ‚àödk ) * V`. Understand what each matrix represents and why each operation (dot product, scaling, softmax, multiplication by V) is performed.\n",
      "\n",
      "**Step 3: Build Up the Architecture Piece by Piece**\n",
      "*   **Multi-Head Attention:** Once you understand single attention, grasp why doing it multiple times in parallel (\"multiple heads\") is beneficial. It's like asking a question to a group of experts who each focus on a different aspect of the problem.\n",
      "*   **Positional Encoding:** The Transformer has no built-in sense of word order like an RNN. Learn how Positional Encodings are added to the input embeddings to give the model information about the position of each token in the sequence.\n",
      "*   **The Encoder Block:** Understand how data flows through a single encoder layer: Input -> Self-Attention -> Add & Norm -> Feed Forward -> Add & Norm -> Output.\n",
      "*   **The Decoder Block:** Understand the three sub-layers in the decoder:\n",
      "    1.  **Masked Self-Attention:** Why the masking is necessary for generating sequences one token at a time.\n",
      "    2.  **Encoder-Decoder Attention:** How the decoder uses its queries to attend to the keys and values from the final encoder layer's output. This is how the model connects the input and output sequences.\n",
      "    3.  **Feed Forward Network:** The final processing step in the layer.\n",
      "\n",
      "**Step 4: Use Practical Resources and Code**\n",
      "*   **Read the Paper:** The context you provided is from the original paper, \"Attention Is All You Need.\" Reading it is a great starting point.\n",
      "*   **Visual Explanations:** Search for resources like Jay Alammar's \"The Illustrated Transformer,\" which provides excellent, intuitive visualizations of how the model works.\n",
      "*   **Implement It:** The best way to truly understand the model is to build it from scratch using a framework like PyTorch or TensorFlow. This forces you to engage with every detail, from matrix dimensions to the masking logic.\n"
     ]
    }
   ],
   "source": [
    "def ask_question(query: str, vectorstore_or_path=r\"D:\\MultiModulRag\\Backend\\Pipeline_Database\\chroma_db\", k: int = 2):\n",
    "    \"\"\"\n",
    "    Ask a question and get answer with context\n",
    "    \n",
    "    Args:\n",
    "        query: User question\n",
    "        vectorstore_or_path: Either ChromaDB vectorstore object or path to persisted DB\n",
    "        k: Number of chunks to retrieve\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Load vectorstore if path is provided\n",
    "    if isinstance(vectorstore_or_path, (str, Path)):\n",
    "        print(f\"üìÇ Loading vector store from: {vectorstore_or_path}\")\n",
    "        embedding_model = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\")\n",
    "        vectorstore = Chroma(\n",
    "            persist_directory=str(vectorstore_or_path),\n",
    "            embedding_function=embedding_model\n",
    "        )\n",
    "    else:\n",
    "        vectorstore = vectorstore_or_path\n",
    "    \n",
    "    print(f\"üîç Searching for: {query}\\n\")\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    results = vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "    # Parse metadata and build context\n",
    "    context_parts = []\n",
    "    all_images = []\n",
    "    \n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"üìÑ Chunk {i}:\")\n",
    "        print(f\"   Pages: {json.loads(doc.metadata['page_numbers'])}\")\n",
    "        print(f\"   Types: {json.loads(doc.metadata['content_types'])}\")\n",
    "        \n",
    "        # Get metadata\n",
    "        original_text = doc.metadata['original_text']\n",
    "        tables = json.loads(doc.metadata['raw_tables_html'])\n",
    "        images = json.loads(doc.metadata['image_paths'])\n",
    "        \n",
    "        # Build context from metadata\n",
    "        chunk_context = f\"Context {i}:\\n\"\n",
    "        chunk_context += f\"Text: {original_text}\\n\"\n",
    "        \n",
    "        if tables:\n",
    "            print(f\"   üìä Tables: {len(tables)}\")\n",
    "            chunk_context += f\"\\nTables:\\n\"\n",
    "            for j, table in enumerate(tables, 1):\n",
    "                chunk_context += f\"Table {j}:\\n{table}\\n\"\n",
    "        \n",
    "        if images:\n",
    "            print(f\"   üñºÔ∏è Images: {len(images)}\")\n",
    "            all_images.extend(images)\n",
    "        \n",
    "        context_parts.append(chunk_context)\n",
    "        print()\n",
    "    \n",
    "    # Combine context\n",
    "    combined_context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Generate answer\n",
    "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-pro\", temperature=0)\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "Context:\n",
    "{combined_context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    print(\"ü§ñ Generating answer...\\n\")\n",
    "    response = llm.invoke(prompt)\n",
    "    \n",
    "    print(\"üí° Answer:\")\n",
    "    print(response.content)\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "# # Usage - Both work now:\n",
    "# # Option 1: Pass vectorstore object\n",
    "# answer = ask_question(\"What is the revenue?\")\n",
    "\n",
    "# Option 2: Pass path to persisted DB\n",
    "answer = ask_question(\"What is this pdf about and how to learn it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b8b9205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Based on the provided context, here is an answer to your question.\\n'\n",
      " '\\n'\n",
      " '### What is this PDF about?\\n'\n",
      " '\\n'\n",
      " 'This document describes the **Transformer model architecture**, a highly '\n",
      " 'influential neural network design primarily used for natural language '\n",
      " 'processing (NLP) and other sequence-to-sequence tasks. The paper it '\n",
      " 'originates from is titled \"Attention Is All You Need.\"\\n'\n",
      " '\\n'\n",
      " 'Based on the context, the key concepts covered are:\\n'\n",
      " '\\n'\n",
      " '1.  **The Core Mechanism: Scaled Dot-Product Attention:** This is the '\n",
      " \"fundamental building block of the Transformer. It's a mechanism that allows \"\n",
      " 'the model to weigh the importance of different words (or parts of a '\n",
      " 'sequence) when processing a particular word.\\n'\n",
      " '    *   It operates on a set of **queries (Q)**, **keys (K)**, and **values '\n",
      " '(V)**.\\n'\n",
      " '    *   The attention weights are calculated using the formula: '\n",
      " '`Attention(Q,K,V) = softmax( (Q * K^T) / ‚àödk ) * V`.\\n'\n",
      " '    *   The scaling factor (`1/‚àödk`) is crucial for stabilizing the training '\n",
      " 'process, especially when the dimensions (`dk`) are large, by preventing the '\n",
      " 'softmax function from having extremely small gradients.\\n'\n",
      " '\\n'\n",
      " '2.  **Multi-Head Attention:** This is an enhancement to the basic attention '\n",
      " 'mechanism. Instead of performing attention once, it runs multiple \"Scaled '\n",
      " 'Dot-Product Attention\" operations in parallel (the \"heads\"). This allows the '\n",
      " 'model to jointly attend to information from different representational '\n",
      " 'subspaces at different positions, capturing a richer variety of '\n",
      " 'relationships.\\n'\n",
      " '\\n'\n",
      " '3.  **The Overall Architecture:** The document outlines the full '\n",
      " 'encoder-decoder structure of the Transformer model.\\n'\n",
      " '    *   **Encoder and Decoder Stacks:** The model is composed of a stack of '\n",
      " 'encoders and a stack of decoders (typically N=6 of each).\\n'\n",
      " '    *   **Sub-layers:** Each layer contains sub-layers, including Multi-Head '\n",
      " 'Attention and a Feed-Forward Network.\\n'\n",
      " '    *   **Residual Connections and Layer Normalization:** To improve '\n",
      " 'training stability and information flow, the model uses residual connections '\n",
      " '(`x + Sublayer(x)`) followed by layer normalization (`LayerNorm(...)`) '\n",
      " 'around each sub-layer.\\n'\n",
      " '    *   **Decoder-Specific Attention:** The decoder has a special \"masked\" '\n",
      " 'multi-head self-attention sub-layer to ensure that when predicting a word, '\n",
      " 'it can only attend to previous words in the sequence. It also has a third '\n",
      " \"sub-layer that performs attention over the encoder's output, allowing it to \"\n",
      " 'focus on relevant parts of the input sequence.\\n'\n",
      " '\\n'\n",
      " '---\\n'\n",
      " '\\n'\n",
      " '### How to learn it?\\n'\n",
      " '\\n'\n",
      " 'Learning the Transformer architecture can be broken down into a structured, '\n",
      " 'step-by-step process.\\n'\n",
      " '\\n'\n",
      " '**Step 1: Master the Prerequisites**\\n'\n",
      " 'Before tackling the Transformer, ensure you have a solid understanding of:\\n'\n",
      " '*   **Linear Algebra:** Especially matrix multiplication, as it is the core '\n",
      " 'operation in the attention formula.\\n'\n",
      " '*   **Neural Network Fundamentals:** Understand concepts like layers, '\n",
      " 'weights, activation functions (especially softmax), and '\n",
      " 'backpropagation/gradients.\\n'\n",
      " '*   **Sequence Models (for context):** Learn about Recurrent Neural Networks '\n",
      " '(RNNs) and LSTMs. Understanding their limitations (e.g., difficulty with '\n",
      " 'long-range dependencies, sequential computation) will clarify *why* the '\n",
      " 'Transformer was a breakthrough.\\n'\n",
      " '\\n'\n",
      " '**Step 2: Understand the Core Concept: Attention**\\n'\n",
      " '*   **Intuition:** Start with the high-level idea of attention. Think of it '\n",
      " \"as a search or a lookup. For a given **query** (e.g., the word we're \"\n",
      " 'currently processing), we compare it against a set of **keys** (e.g., all '\n",
      " 'other words in the sentence) to find how relevant they are. We then use '\n",
      " 'these relevance scores to create a weighted sum of the **values** (the '\n",
      " 'information from those other words).\\n'\n",
      " '*   **Scaled Dot-Product Attention:** Carefully walk through the formula '\n",
      " '`softmax( (Q * K^T) / ‚àödk ) * V`. Understand what each matrix represents and '\n",
      " 'why each operation (dot product, scaling, softmax, multiplication by V) is '\n",
      " 'performed.\\n'\n",
      " '\\n'\n",
      " '**Step 3: Build Up the Architecture Piece by Piece**\\n'\n",
      " '*   **Multi-Head Attention:** Once you understand single attention, grasp '\n",
      " 'why doing it multiple times in parallel (\"multiple heads\") is beneficial. '\n",
      " \"It's like asking a question to a group of experts who each focus on a \"\n",
      " 'different aspect of the problem.\\n'\n",
      " '*   **Positional Encoding:** The Transformer has no built-in sense of word '\n",
      " 'order like an RNN. Learn how Positional Encodings are added to the input '\n",
      " 'embeddings to give the model information about the position of each token in '\n",
      " 'the sequence.\\n'\n",
      " '*   **The Encoder Block:** Understand how data flows through a single '\n",
      " 'encoder layer: Input -> Self-Attention -> Add & Norm -> Feed Forward -> Add '\n",
      " '& Norm -> Output.\\n'\n",
      " '*   **The Decoder Block:** Understand the three sub-layers in the decoder:\\n'\n",
      " '    1.  **Masked Self-Attention:** Why the masking is necessary for '\n",
      " 'generating sequences one token at a time.\\n'\n",
      " '    2.  **Encoder-Decoder Attention:** How the decoder uses its queries to '\n",
      " \"attend to the keys and values from the final encoder layer's output. This is \"\n",
      " 'how the model connects the input and output sequences.\\n'\n",
      " '    3.  **Feed Forward Network:** The final processing step in the layer.\\n'\n",
      " '\\n'\n",
      " '**Step 4: Use Practical Resources and Code**\\n'\n",
      " '*   **Read the Paper:** The context you provided is from the original paper, '\n",
      " '\"Attention Is All You Need.\" Reading it is a great starting point.\\n'\n",
      " '*   **Visual Explanations:** Search for resources like Jay Alammar\\'s \"The '\n",
      " 'Illustrated Transformer,\" which provides excellent, intuitive visualizations '\n",
      " 'of how the model works.\\n'\n",
      " '*   **Implement It:** The best way to truly understand the model is to build '\n",
      " 'it from scratch using a framework like PyTorch or TensorFlow. This forces '\n",
      " 'you to engage with every detail, from matrix dimensions to the masking '\n",
      " 'logic.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
